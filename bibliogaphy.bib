@online{atilLLMStabilityDetailed2024,
  title = {{{LLM Stability}}: {{A}} Detailed Analysis with Some Surprises},
  shorttitle = {{{LLM Stability}}},
  author = {Atil, Berk and Chittams, Alexa and Fu, Liseng and Ture, Ferhan and Xu, Lixinyu and Baldwin, Breck},
  date = {2024-09-12},
  eprint = {2408.04667},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2408.04667},
  url = {http://arxiv.org/abs/2408.04667},
  urldate = {2025-03-02},
  abstract = {LLM (large language model) practitioners commonly notice that outputs can vary for the same inputs, but we have been unable to find work that evaluates LLM stability as the main objective. In our study of 6 deterministically configured LLMs across 8 common tasks with 5 identical runs, we see accuracy variations up to 10\textbackslash\%. In addition, no LLM consistently delivers repeatable accuracy across all tasks. We also show examples of variation that are not normally distributed and compare configurations with zero-shot/few-shot prompting and fine-tuned examples. To better quantify what is going on, we introduce metrics focused on stability: TARr@N for the total agreement rate at N runs over raw output, and TARa@N for total agreement over parsed-out answers. We suggest that stability metrics be integrated into leader boards and research results going forward.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Software Engineering},
  file = {/home/s0pex/Zotero/storage/F9EGQH9X/Atil et al. - 2024 - LLM Stability A detailed analysis with some surprises.pdf;/home/s0pex/Zotero/storage/7H9NKS9X/2408.html}
}

@online{blair-stanekLLMsProvideUnstable2025,
  title = {{{LLMs Provide Unstable Answers}} to {{Legal Questions}}},
  author = {Blair-Stanek, Andrew and Durme, Benjamin Van},
  date = {2025-01-28},
  eprint = {2502.05196},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2502.05196},
  url = {http://arxiv.org/abs/2502.05196},
  urldate = {2025-03-02},
  abstract = {An LLM is stable if it reaches the same conclusion when asked the identical question multiple times. We find leading LLMs like gpt-4o, claude-3.5, and gemini-1.5 are unstable when providing answers to hard legal questions, even when made as deterministic as possible by setting temperature to 0. We curate and release a novel dataset of 500 legal questions distilled from real cases, involving two parties, with facts, competing legal arguments, and the question of which party should prevail. When provided the exact same question, we observe that LLMs sometimes say one party should win, while other times saying the other party should win. This instability has implications for the increasing numbers of legal AI products, legal processes, and lawyers relying on these LLMs.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computation and Language,Computer Science - Computers and Society},
  file = {/home/s0pex/Zotero/storage/4G98DWCX/Blair-Stanek and Durme - 2025 - LLMs Provide Unstable Answers to Legal Questions.pdf;/home/s0pex/Zotero/storage/Z2T5GJVI/2502.html}
}

@online{brownLanguageModelsAre2020,
  title = {Language {{Models}} Are {{Few-Shot Learners}}},
  author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
  date = {2020-07-22},
  eprint = {2005.14165},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2005.14165},
  url = {http://arxiv.org/abs/2005.14165},
  urldate = {2025-03-02},
  abstract = {Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computation and Language},
  file = {/home/s0pex/Zotero/storage/LTAT38WS/Brown et al. - 2020 - Language Models are Few-Shot Learners.pdf;/home/s0pex/Zotero/storage/T4X7Q3YD/2005.html}
}

@online{chenWeakevalStrongEvaluatingEliciting2024,
  title = {Weak-Eval-{{Strong}}: {{Evaluating}} and {{Eliciting Lateral Thinking}} of {{LLMs}} with {{Situation Puzzles}}},
  shorttitle = {Weak-Eval-{{Strong}}},
  author = {Chen, Qi and Zhang, Bowen and Wang, Gang and Wu, Qi},
  date = {2024-10-09},
  eprint = {2410.06733},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2410.06733},
  url = {http://arxiv.org/abs/2410.06733},
  urldate = {2025-03-04},
  abstract = {While advancements in NLP have significantly improved the performance of Large Language Models (LLMs) on tasks requiring vertical thinking, their lateral thinking capabilities remain under-explored and challenging to measure due to the complexity of assessing creative thought processes and the scarcity of relevant data. To address these challenges, we introduce SPLAT, a benchmark leveraging Situation Puzzles to evaluate and elicit LAteral Thinking of LLMs. This benchmark, containing 975 graded situation puzzles across three difficulty levels, employs a new multi-turn player-judge framework instead of the traditional model-based evaluation, which often necessitates a stronger evaluation model. This framework simulates an interactive game where the model (player) asks the evaluation model (judge) questions about an incomplete story to infer the full scenario. The judge answers based on a detailed reference scenario or evaluates if the player's predictions align with the reference one. This approach lessens dependence on more robust evaluation models, enabling the assessment of state-of-the-art LLMs. The experiments demonstrate that a robust evaluation model, such as WizardLM-2, closely matches human judgements in both intermediate question-answering and final scenario accuracy, achieving over 80\% agreement-similar to the agreement levels among humans. Furthermore, applying data and reasoning processes from our benchmark to other lateral thinking-related benchmarks, e.g., RiddleSense and BrainTeaser, leads to performance enhancements. This suggests that our benchmark effectively evaluates and elicits the lateral thinking abilities of LLMs. Code is available at: https://github.com/chenqi008/LateralThinking.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/s0pex/Zotero/storage/ZMVZR398/Chen et al. - 2024 - Weak-eval-Strong Evaluating and Eliciting Lateral Thinking of LLMs with Situation Puzzles.pdf;/home/s0pex/Zotero/storage/2UNMP3YM/2410.html}
}

@article{haoRecentProgressLeveraging2022,
  title = {Recent Progress in Leveraging Deep Learning Methods for Question Answering},
  author = {Hao, Tianyong and Li, Xinxin and He, Yulan and Wang, Fu Lee and Qu, Yingying},
  date = {2022-02-01},
  journaltitle = {Neural Comput. Appl.},
  volume = {34},
  number = {4},
  pages = {2765--2783},
  issn = {0941-0643},
  doi = {10.1007/s00521-021-06748-3},
  url = {https://doi.org/10.1007/s00521-021-06748-3},
  urldate = {2025-03-04},
  abstract = {Question answering, serving as one of important tasks in natural language processing, enables machines to understand questions in natural language and answer the questions concisely. From web search to expert systems, question answering systems are widely applied to various domains in assisting information seeking. Deep learning methods have boosted various tasks of question answering and have demonstrated dramatic effects in performance improvement for essential steps of question answering. Thus, leveraging deep learning methods for question answering has drawn much attention from both academia and industry in recent years. This paper provides a systematic review of the recent development of deep learning methods for question answering. The survey covers the scope including methods, datasets, and applications. The methods are discussed in terms of network structure characteristics, methodology innovations, and their effectiveness. The survey is expected to be a contribution to the summarization of recent research progress and future directions of deep learning methods for question answering.}
}

@inproceedings{huangLatEvalInteractiveLLMs2024,
  title = {{{LatEval}}: {{An Interactive LLMs Evaluation Benchmark}} with {{Incomplete Information}} from {{Lateral Thinking Puzzles}}},
  shorttitle = {{{LatEval}}},
  booktitle = {Proceedings of the 2024 {{Joint International Conference}} on {{Computational Linguistics}}, {{Language Resources}} and {{Evaluation}} ({{LREC-COLING}} 2024)},
  author = {Huang, Shulin and Ma, Shirong and Li, Yinghui and Huang, Mengzuo and Zou, Wuhe and Zhang, Weidong and Zheng, Haitao},
  editor = {Calzolari, Nicoletta and Kan, Min-Yen and Hoste, Veronique and Lenci, Alessandro and Sakti, Sakriani and Xue, Nianwen},
  date = {2024-05},
  pages = {10186--10197},
  publisher = {{ELRA and ICCL}},
  location = {Torino, Italia},
  url = {https://aclanthology.org/2024.lrec-main.889/},
  urldate = {2025-03-04},
  abstract = {With the evolution of LLMs, they are endowed with impressive logical reasoning, or vertical thinking capabilities. But can they think out of the box? Do they possess proficient lateral thinking abilities? Following the setup of Lateral Thinking Puzzles, we propose a novel evaluation benchmark, LatEval, which assesses the model`s lateral thinking within an interactive framework. In our benchmark, we challenge LLMs with 2 aspects: (1) posing high-quality questions that break out of conventional norms but are beneficial for puzzle-solving. (2) integrating existing information to gradually deduce the truth through reasoning. We observe that it is hard for most LLMs to accomplish lateral thinking during interactions. Even the most powerful LLM, GPT-4, faces challenges in achieving satisfactory performance, and for most open-source models, simply completing this task is quite difficult. This evaluation benchmark provides LLMs with a highly challenging and differentiating task that is crucial to an effective AI assistant. Our dataset and source codes are available at https://github.com/THUKElab/LatEval.},
  eventtitle = {{{LREC-COLING}} 2024},
  file = {/home/s0pex/Zotero/storage/JQ3I8DYK/Huang et al. - 2024 - LatEval An Interactive LLMs Evaluation Benchmark with Incomplete Information from Lateral Thinking.pdf}
}

@online{jiangBRAINTEASERLateralThinking2023,
  title = {{{BRAINTEASER}}: {{Lateral Thinking Puzzles}} for {{Large Language Models}}},
  shorttitle = {{{BRAINTEASER}}},
  author = {Jiang, Yifan and Ilievski, Filip and Ma, Kaixin and Sourati, Zhivar},
  date = {2023-11-09},
  eprint = {2310.05057},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2310.05057},
  url = {http://arxiv.org/abs/2310.05057},
  urldate = {2025-03-02},
  abstract = {The success of language models has inspired the NLP community to attend to tasks that require implicit and complex reasoning, relying on human-like commonsense mechanisms. While such vertical thinking tasks have been relatively popular, lateral thinking puzzles have received little attention. To bridge this gap, we devise BRAINTEASER: a multiple-choice Question Answering task designed to test the model's ability to exhibit lateral thinking and defy default commonsense associations. We design a three-step procedure for creating the first lateral thinking benchmark, consisting of data collection, distractor generation, and generation of adversarial examples, leading to 1,100 puzzles with high-quality annotations. To assess the consistency of lateral reasoning by models, we enrich BRAINTEASER based on a semantic and contextual reconstruction of its questions. Our experiments with state-of-the-art instruction- and commonsense language models reveal a significant gap between human and model performance, which is further widened when consistency across adversarial formats is considered. We make all of our code and data available to stimulate work on developing and evaluating lateral thinking models.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computation and Language},
  file = {/home/s0pex/Zotero/storage/2NLDXC5P/Jiang et al. - 2023 - BRAINTEASER Lateral Thinking Puzzles for Large Language Models.pdf;/home/s0pex/Zotero/storage/N2UZXVVU/2310.html}
}

@inproceedings{liHWTSCSemEval2024Task2024,
  title = {{{HW-TSC}} at {{SemEval-2024 Task}} 9: {{Exploring Prompt Engineering Strategies}} for {{Brain Teaser Puzzles Through LLMs}}},
  shorttitle = {{{HW-TSC}} at {{SemEval-2024 Task}} 9},
  booktitle = {Proceedings of the 18th {{International Workshop}} on {{Semantic Evaluation}} ({{SemEval-2024}})},
  author = {Li, Yinglu and Yanqing, Zhao and Zhang, Min and Deng, Yadong and Geng, Aiju and Liu, Xiaoqin and Ren, Mengxin and Li, Yuang and Chang, Su and Zhao, Xiaofeng},
  editor = {Ojha, Atul Kr. and Doğruöz, A. Seza and Tayyar Madabushi, Harish and Da San Martino, Giovanni and Rosenthal, Sara and Rosá, Aiala},
  date = {2024-06},
  pages = {1646--1651},
  publisher = {Association for Computational Linguistics},
  location = {Mexico City, Mexico},
  doi = {10.18653/v1/2024.semeval-1.234},
  url = {https://aclanthology.org/2024.semeval-1.234/},
  urldate = {2025-03-02},
  abstract = {Large Language Models (LLMs) have demonstrated impressive performance on many Natural Language Processing (NLP) tasks. However, their ability to solve more creative, lateral thinking puzzles remains relatively unexplored. In this work, we develop methods to enhance the lateral thinking and puzzle-solving capabilities of LLMs. We curate a dataset of word-type and sentence-type brain teasers requiring creative problem-solving abilities beyond commonsense reasoning. We first evaluate the zero-shot performance of models like GPT-3.5 and GPT-4 on this dataset. To improve their puzzle-solving skills, we employ prompting techniques like providing reasoning clues and chaining multiple examples to demonstrate the desired thinking process. We also fine-tune the state-of-the-art Mixtral 7x8b LLM on ourdataset. Our methods enable the models to achieve strong results, securing 2nd and 3rd places in the brain teaser task. Our work highlights the potential of LLMs in acquiring complex reasoning abilities with the appropriate training. The efficacy of our approaches opens up new research avenues into advancing lateral thinking and creative problem-solving with AI systems.},
  eventtitle = {{{SemEval}} 2024},
  file = {/home/s0pex/Zotero/storage/58AY26BM/Li et al. - 2024 - HW-TSC at SemEval-2024 Task 9 Exploring Prompt Engineering Strategies for Brain Teaser Puzzles Thro.pdf}
}

@online{OpenLLMLeaderboard,
  title = {Open {{LLM Leaderboard}} - a {{Hugging Face Space}} by Open-Llm-Leaderboard},
  url = {https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard},
  urldate = {2025-03-04},
  abstract = {Track, rank and evaluate open LLMs and chatbots},
  file = {/home/s0pex/Zotero/storage/WQTXSUTE/open_llm_leaderboard.html}
}

@online{panLogicLMEmpoweringLarge2023,
  title = {Logic-{{LM}}: {{Empowering Large Language Models}} with {{Symbolic Solvers}} for {{Faithful Logical Reasoning}}},
  shorttitle = {Logic-{{LM}}},
  author = {Pan, Liangming and Albalak, Alon and Wang, Xinyi and Wang, William Yang},
  date = {2023-10-19},
  eprint = {2305.12295},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2305.12295},
  url = {http://arxiv.org/abs/2305.12295},
  urldate = {2025-03-04},
  abstract = {Large Language Models (LLMs) have shown human-like reasoning abilities but still struggle with complex logical problems. This paper introduces a novel framework, Logic-LM, which integrates LLMs with symbolic solvers to improve logical problem-solving. Our method first utilizes LLMs to translate a natural language problem into a symbolic formulation. Afterward, a deterministic symbolic solver performs inference on the formulated problem. We also introduce a self-refinement module, which utilizes the symbolic solver's error messages to revise symbolic formalizations. We demonstrate Logic-LM's effectiveness on five logical reasoning datasets: ProofWriter, PrOntoQA, FOLIO, LogicalDeduction, and AR-LSAT. On average, Logic-LM achieves a significant performance boost of 39.2\% over using LLM alone with standard prompting and 18.4\% over LLM with chain-of-thought prompting. Our findings suggest that Logic-LM, by combining LLMs with symbolic logic, offers a promising avenue for faithful logical reasoning. Code and data are publicly available at https://github.com/teacherpeterpan/Logic-LLM.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/home/s0pex/Zotero/storage/HYPF8KH4/Pan et al. - 2023 - Logic-LM Empowering Large Language Models with Symbolic Solvers for Faithful Logical Reasoning.pdf;/home/s0pex/Zotero/storage/3RJYU93I/2305.html}
}

@online{yinDeeperInsightsUpdates2024,
  title = {Deeper {{Insights Without Updates}}: {{The Power}} of {{In-Context Learning Over Fine-Tuning}}},
  shorttitle = {Deeper {{Insights Without Updates}}},
  author = {Yin, Qingyu and He, Xuzheng and Deng, Luoao and Leong, Chak Tou and Wang, Fan and Yan, Yanzhao and Shen, Xiaoyu and Zhang, Qiang},
  date = {2024-10-07},
  eprint = {2410.04691},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2410.04691},
  url = {http://arxiv.org/abs/2410.04691},
  urldate = {2025-03-04},
  abstract = {Fine-tuning and in-context learning (ICL) are two prevalent methods in imbuing large language models with task-specific knowledge. It is commonly believed that fine-tuning can surpass ICL given sufficient training samples as it allows the model to adjust its internal parameters based on the data. However, this paper presents a counterintuitive finding: For tasks with implicit patterns, ICL captures these patterns significantly better than fine-tuning. We developed several datasets featuring implicit patterns, such as sequences determining answers through parity or identifying reducible terms in calculations. We then evaluated the models' understanding of these patterns under both fine-tuning and ICL across models ranging from 0.5B to 7B parameters. The results indicate that models employing ICL can quickly grasp deep patterns and significantly improve accuracy. In contrast, fine-tuning, despite utilizing thousands of times more training samples than ICL, achieved only limited improvements. We also proposed circuit shift theory from a mechanistic interpretability's view to explain why ICL wins.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/s0pex/Zotero/storage/2FCLHPE3/Yin et al. - 2024 - Deeper Insights Without Updates The Power of In-Context Learning Over Fine-Tuning.pdf;/home/s0pex/Zotero/storage/AFELEWRD/2410.html}
}

@online{yuFlowReasoningTraining2025,
  title = {Flow of {{Reasoning}}:{{Training LLMs}} for {{Divergent Problem Solving}} with {{Minimal Examples}}},
  shorttitle = {Flow of {{Reasoning}}},
  author = {Yu, Fangxu and Jiang, Lai and Kang, Haoqiang and Hao, Shibo and Qin, Lianhui},
  date = {2025-02-21},
  eprint = {2406.05673},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2406.05673},
  url = {http://arxiv.org/abs/2406.05673},
  urldate = {2025-03-04},
  abstract = {The ability to generate diverse solutions to a given problem is a hallmark of human creativity. This divergent reasoning is also crucial for machines, enhancing their robustness and enabling them to assist humans in many applications such as scientific discovery. However, existing approaches to multi-step reasoning with large language models (LLMs) have mostly focused only on reasoning accuracy, without further discovering more diverse valid solutions. For example, supervised fine-tuning can improve LLM reasoning quality, but requires extensive supervised data to capture the full range of possible solutions. Reward-maximization reinforcement learning aims to find limited highest-reward solutions while neglecting the solution diversity. To fill this gap, we propose Flow of Reasoning (FoR), an efficient diversity-seeking LLM finetuning method aimed at improving reasoning quality and diversity with minimal data. FoR formulates multi-step LLM reasoning as a Markovian flow on a DAG-structured reasoning graph. This formulation allows us to incorporate and adapt principled GFlowNet approaches, for finetuning LLMs to sample divergent paths with probabilities proportional to the (unnormalized) reward of target problems. Extensive experiments show that, with limited training examples (e.g., 15 examples), FoR enables the discovery of diverse, creative, high-quality solutions, greatly outperforming a wide range of existing inference and training methods across six challenging reasoning tasks, including BlocksWorld (embodied reasoning), Game24 (math puzzle solving), Rubik's Cube (spatial reasoning), 1D-ARC (abstraction reasoning), GSM8k (math reasoning), and ProntoQA (logical reasoning). Code is available at https://github.com/Yu-Fangxu/FoR.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/home/s0pex/Zotero/storage/L84YWW26/Yu et al. - 2025 - Flow of ReasoningTraining LLMs for Divergent Problem Solving with Minimal Examples.pdf;/home/s0pex/Zotero/storage/KRL3WB7I/2406.html}
}

@inproceedings{zebazeTreeProblemsImproving2024,
  title = {Tree of {{Problems}}: {{Improving}} Structured Problem Solving with Compositionality},
  shorttitle = {Tree of {{Problems}}},
  booktitle = {Proceedings of the 2024 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Zebaze, Armel Randy and Sagot, Benoît and Bawden, Rachel},
  editor = {Al-Onaizan, Yaser and Bansal, Mohit and Chen, Yun-Nung},
  date = {2024-11},
  pages = {18028--18047},
  publisher = {Association for Computational Linguistics},
  location = {Miami, Florida, USA},
  doi = {10.18653/v1/2024.emnlp-main.1001},
  url = {https://aclanthology.org/2024.emnlp-main.1001/},
  urldate = {2025-03-04},
  abstract = {Large Language Models (LLMs) have demonstrated remarkable performance across multipletasks through in-context learning. For complex reasoning tasks that require step-by-step thinking, Chain-of-Thought (CoT) prompting has given impressive results, especially when combined with self-consistency. Nonetheless, some tasks remain particularly difficult for LLMs to solve. Tree of Thoughts (ToT) and Graph of Thoughts (GoT) emerged as alternatives, dividing the complex problem into paths of subproblems. In this paper, we propose Tree of Problems (ToP), a simpler version of ToT, which we hypothesise can work better for complex tasks that can be divided into identical subtasks. Our empirical results show that our approach outperforms ToT and GoT, and in addition per forms better than CoT on complex reasoning tasks. All code for this paper will be made available.},
  eventtitle = {{{EMNLP}} 2024},
  file = {/home/s0pex/Zotero/storage/W9KX44FW/Zebaze et al. - 2024 - Tree of Problems Improving structured problem solving with compositionality.pdf}
}
