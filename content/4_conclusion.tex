This paper investigated the performance of various open-source large language models on the lateral thinking puzzles challenge, \textit{SemEval 2024 Task 4: BrainTeaser}~\cite{jiangBRAINTEASERLateralThinking2023}. Experiments were conducted using both raw prompts (baseline) and optimized approaches, testing Prompt Engineering and \acf{ICL}, i.e., few-shot prompting. The results reveal that \ac{ICL} significantly improved model performance, especially with larger models, achieving up to a 23\% increase in raw accuracy. In contrast, prompt engineering, particularly system prompt optimization, led to a decrease in raw performance due to excessive auxiliary text. However, post-processed results, which used simple algorithms to extract answers from model outputs, showed an improvement of up to 7\% compared to the baseline.

Upon analyzing the low performance when applying prompt engineering, it was observed that techniques such as instructing the model to think step-by-step or consider different perspectives resulted in models outputting excessive auxiliary information—like explanations—before the actual answer. This caused a decrease in raw performance scores. This suggests that raw model responses alone do not fully capture a model's capabilities, as output format inconsistencies can negatively impact results. Post-processing, however, provided a clearer and more accurate picture of model performance.

The experiments further highlighted that \ac{ICL} helps align model outputs with the desired format, leading to significant improvements in raw accuracy. Notably, when using the best-performing model, \textit{Phi4:14B}~\cite{abdinPhi4TechnicalReport2024}, with simple, randomly selected examples from the dataset, \ac{ICL} achieved competitive performance compared to closed models like ChatGPT-4~\cite{openaiGPT4TechnicalReport2024}, which were used by other competition participants.

However, comparing these experiments with those of other participants, it is evident that selecting random examples for few-shot learning is not optimal. Other participants likely employed more sophisticated techniques to extract examples. Future research should explore methods such as selecting examples based on embedding vectors rather than random selection to improve performance further.

Additionally, the 24 GB of \acf{VRAM} available for model selection in this paper was a limiting factor, which may have hindered the performance of models compared to those used by other participants. Some models, such as LLaMA 405B, which were employed by other competitors, are far superior to those available here, suggesting that model size may have played a significant role in performance disparities.

For future work, several promising directions emerge. First, refining techniques for managing verbose model outputs could further enhance performance. Second, exploring advanced strategies like model temperature tuning and \ac{CoT} prompting could provide additional improvements in accuracy. Finally, a deeper comparative analysis between similarly sized open-source and closed-source models could offer valuable insights into a potentially more accurate and fair comparison between the employed models.
