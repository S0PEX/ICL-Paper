This paper examined the performance of various \aclp{oLLM} (open LLMs) on the lateral thinking puzzles challenge, \textit{SemEval 2024 Task 4: BrainTeaser}~\cite{jiangBRAINTEASERLateralThinking2023}. Experiments were conducted to compare multiple models in both baseline and optimized settings, utilizing system prompt engineering techniques and \acf{ICL}~\cite{brownLanguageModelsAre2020} to potentially improve performance. The results indicate that \ac{ICL} significantly enhanced model performance across the board, yielding up to a 23\% increase in raw accuracy. In contrast, prompt engineering, specifically to system prompts, reduced raw performance due to excessive auxiliary text in model responses. However, post-processing, which was able to extract model answers from verbose outputs, improved performance by up to 7\% compared to the baseline when prompt engineering was applied.

A deeper analysis of the performance degradation caused by prompt engineering revealed that instructing models to think step-by-step or consider different perspectives led to excessive auxiliary information, such as explanations, preceding the actual answer, which lowered raw performance scores. This suggests that evaluating such benchmarks on raw responses alone does not fully reflect a model's capabilities, as inconsistencies in output format negatively impacted results. Post-processing, however, provided a more accurate representation of model performance.

The experiments further demonstrated that \ac{ICL} helped align model outputs with the expected answer format required by the benchmark, significantly boosting accuracy. Notably, the best-performing model in this paper, Gemma2:27B-(SPO-FS\footnote{SPO-FS refers to \underline{S}ystem-\underline{P}rompt-\underline{O}ptimized \underline{F}ew-\underline{S}hot Prompting}) (80.45\% on \ac{SP} and 67.28\% on \ac{WP}), would have ranked 15th out of 30 on \ac{SP} and approximately 16th on \ac{WP} on the \href{https://brainteasersem.github.io/\#leaderboard}{SemEval leaderboard}. It achieved competitive performance against closed models like ChatGPT-4~\cite{openaiGPT4TechnicalReport2024}.

However, a comparison with other participants' approaches suggests that the techniques employed in the experiments, especially the selection of random examples in the few-shot experiments, are suboptimal~\cite{jiangSemEval2024Task92024}. More sophisticated selection methods contributed to their superior performance, as these participants primarily relied on a variety of advanced example selection methods in their few-shot strategies~\cite{jiangSemEval2024Task92024}. Therefore, future work should explore advanced strategies, such as selecting examples based on similarity search between examples and target questions~\cite{HowUseFew}.

Additionally, the 24 GB of \ac{VRAM} available for model selection in this paper posed a limitation, potentially restricting model performance relative to competitors. Some participants utilized models like ChatGPT, which are on par with LLaMA 3 405B, surpassing the capabilities of the models used in this paper and highlighting the impact of model size on performance disparities~\cite{grattafioriLlama3Herd2024, openaiGPT4TechnicalReport2024}.

Based on these observations, several promising directions for future work emerge. Firstly, as the postprocessed results suggest, one key area for improvement lies in refining the methods used to extract puzzle answers from verbose model outputs. Although prompted to output only the choice, models tend to prefix answers with explanations, especially when instructed to \textit{Think Step-By-Step}.

In addition, exploring advanced strategies such as model temperature tuning and \ac{CoT} prompting, as proposed by \textcite{weiChainofThoughtPromptingElicits2023}, holds great potential for enhancing accuracy. Most participants demonstrated significant performance gains with temperature tuning and \ac{CoT} prompting. Actively encouraging models to break down complex tasks into a series of logical steps before answering, resulting in a substantial improvement in performance, especially when combined with \ac{ICL}.

Finally, conducting a deeper comparative analysis between similarly sized open-source and closed-source models in a more controlled setting would provide valuable insights into their respective strengths and limitations beyond the scope of this paper. The current setup has not accounted for varying model seeds between prompts and other factors, meaning these results should only be viewed as an initial direction. Ideally, a more sophisticated experimental design could offer more comprehensive and accurate assessments.

Nonetheless, this experiment has demonstrated that, while closed-source models often come with proprietary advantages, open-source models are increasingly competitive in lateral thinking tasks.
