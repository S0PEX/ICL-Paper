This paper examined the performance of various \aclp{oLLM} (open LLMs) on the lateral thinking puzzles challenge, \textit{SemEval 2024 Task 4: BrainTeaser}~\cite{jiangBRAINTEASERLateralThinking2023}. Experiments were conducted to compare multiple models in both baseline and optimized settings, utilizing system prompt engineering technique and \acf{ICL}~\cite{brownLanguageModelsAre2020} to potentially improve performance. The results indicate that \ac{ICL} significantly enhanced model performance, particularly for larger models, yielding up to a 23\% increase in raw accuracy. In contrast, prompt engineering, specifically to system prompts, reduced raw performance due to excessive auxiliary text. However, post-processing techniques, which are able to extract model answers from verbose model outputs, improved performance by up to 7\% compared to the baseline when prompt engineering was applied.

A deeper analysis of the performance degradation caused by prompt engineering revealed that instructing models to think step-by-step or consider different perspectives led to excessive auxiliary information, such as explanations, preceding the actual answer, which lowered raw performance scores. This suggests that evaluating such benchmarks on raw responses alone does not fully reflect a model's capabilities, as inconsistencies in output format negatively impacted results. Post-processing, however, provided a more accurate representation of model performance.

The experiments further demonstrated that \ac{ICL} helped align model outputs with the expected answer format required by the benchmark, significantly boosting accuracy. Notably, the best-performing model in this paper, Gemma2:27B-(SPO-FS\footnote{SPO-FS refers to \underline{S}ystem-\underline{P}rompt-\underline{O}ptimized \underline{F}ew-\underline{S}hot Prompting}) (80.45\% on \ac{SP} and 67.28\% on \ac{WP}), would have ranked 15th out of 30 on \ac{SP} and approximately 16th on \ac{WP} on the \href{https://brainteasersem.github.io/\#leaderboard}{SemEval leaderboard}. It achieved competitive performance against closed models like ChatGPT-4~\cite{openaiGPT4TechnicalReport2024}, using simple, randomly selected examples from the dataset.

However, a comparison with other participants' approaches suggests that the techniques employed in the experiments, especially the selection of random examples for few-shot learning, are suboptimal~\cite{jiangSemEval2024Task92024}. More sophisticated selection methods contributed to their superior performance, as these participants primarily relied on a variety of few-shot strategies~\cite{jiangSemEval2024Task92024}. Future work should explore strategies such as example selection based on embedding vectors, rather than random sampling, to further enhance performance in the \ac{ICL} setting, or ideally, utilize the same techniques as the top-performing participants to achieve 1:1 comparable results.

Additionally, the 24 GB of VRAM available for model selection in this paper posed a limitation, potentially restricting model performance relative to competitors. Some participants utilized models like ChatGPT, which are on par with LLaMA 3 405B, surpassing the capabilities of the models used in this paper and highlighting the impact of model size on performance disparities~\cite{grattafioriLlama3Herd2024,openaiGPT4TechnicalReport2024}.

Several promising directions for future work emerge from these findings. Refining techniques for handling verbose model outputs could further improve performance. Additionally, exploring strategies such as model temperature tuning and Chain-of-Thought (CoT) prompting~\cite{weiChainofThoughtPromptingElicits2023} may yield further accuracy improvements. Finally, a deeper comparative analysis between similarly sized open-source and closed-source models could provide a more balanced and fair assessment of model capabilities.
