This paper examined the performance of various open-source large language models on the lateral thinking puzzles challenge, \textit{SemEval 2024 Task 4: BrainTeaser}~\cite{jiangBRAINTEASERLateralThinking2023}. Experiments were conducted using both baseline raw prompts and optimized approaches, including Prompt Engineering and \acf{ICL} (few-shot prompting). The results indicate that \ac{ICL} significantly enhanced model performance, particularly for larger models, yielding up to a 23\% increase in raw accuracy. In contrast, prompt engineering, especially system prompt optimization, reduced raw performance due to excessive auxiliary text. However, post-processing techniques, which extracted answers from model outputs, improved performance by up to 7\% compared to the baseline when applying prompt engineering.

A deeper analysis of the performance degradation caused by prompt engineering revealed that instructing models to think step-by-step or consider different perspectives led to excessive auxiliary information, such as explanations, before the actual answer, which lowered raw performance scores. This suggests that raw outputs alone do not fully reflect a model's capabilities, as inconsistencies in output format negatively impacted results. Post-processing, however, provided a more accurate representation of model performance.

The experiments further demonstrated that \ac{ICL} helped align model outputs with the expected format, significantly boosting raw accuracy. Notably, the best-performing model, \textit{Phi4:14B}~\cite{abdinPhi4TechnicalReport2024}, achieved competitive performance against closed models like ChatGPT-4~\cite{openaiGPT4TechnicalReport2024}, using simple, randomly selected examples from the dataset.

However, a comparison with other participants' approaches suggests that selecting random examples for few-shot learning is suboptimal. More sophisticated selection techniques likely contributed to their superior performance. Future continuation of the experiment should explore strategies such as example selection based on embedding vectors rather than random sampling to further enhance performance in the \ac{ICL} setting.

Additionally, the 24 GB of \acf{VRAM} available for model selection in this paper posed a limitation, potentially restricting model performance relative to competitors. Some participants utilized models, such as ChatGPT, which are on par with LLaMA 3 405B, surpassing the capabilities of the models used in this paper and highlighting the impact of model size on performance disparities.

Several promising directions emerge for future work. Refining techniques for handling verbose model outputs could further enhance performance. Additionally, exploring strategies such as model temperature tuning and \acl{CoT} prompting~\cite{weiChainofThoughtPromptingElicits2023} may lead to further accuracy improvements. Finally, a deeper comparative analysis between similarly sized open-source and closed-source models could provide a more balanced and fair assessment of model capabilities.
