This section outlines the task setup, including the task definition, dataset, and models used in the experiments. It begins with an introduction to the task and dataset, followed by a detailed description of the models employed. The section then emphasizes the experiments conducted, concluding with an analysis of the results obtained.

\subsection{Overview: Task, Dataset, and Models}

\paragraph{Task Definition}
In this experiment, the task setup follows the \textit{SemEval 2024 Task 9: Brainteaser}~\cite{jiangBRAINTEASERLateralThinking2023} challenge, simulating a participant's approach, but with the exclusive use of \acp{oLLM}, in contrast to the original competition, where the majority of participants (90\%) relied on closed-source models, e.g., \acs{GPT}-4~\cite{openaiGPT4TechnicalReport2024}, \acs{GPT}-3.5, and Gemini Pro~\cite{teamGeminiFamilyHighly2024}. As in the official task, evaluation is conducted in a direct question-answering format, where each model is given a puzzle question and is expected to generate the correct answer from a provided list of possible answers. To improve model performance, prompt optimization techniques such as \ac{ICL}~\cite{brownLanguageModelsAre2020} are applied, along with strategies like instructing the model to "think outside the box" or "think step by step". No fine-tuning is performed, with the focus instead on leveraging prompting techniques.

\paragraph{Dataset}
For the dataset, I utilize the original \citetitle{jiangBRAINTEASERLateralThinking2023} dataset by \citeauthor{jiangBRAINTEASERLateralThinking2023}. The dataset, publicly available on \hreffootnote{https://github.com/1171-jpg/BrainTeaser}{GitHub}, consists of 1,100 lateral thinking puzzles formatted as question-answer pairs. The puzzles are categorized into \textit{Sentence puzzles}, where the puzzle defying commonsense is centered on sentence snippets, and \textit{Word puzzles}, where the answer violates the default meaning of the word and focuses on the letter composition of the target question~\cite{jiangBRAINTEASERLateralThinking2023}. Furthermore, the puzzles are organized into sets that include original questions as well as their semantic and contextual variations. This structure allows for a systematic assessment of the models' ability to maintain consistent reasoning across different puzzle formulations~\cite{jiangBRAINTEASERLateralThinking2023}.

Note that while the \textit{SemEval 2024 Task 9} provides an alternative dataset with a predefined training-evaluation split, this experiment follows the original study's approach~\cite{jiangBRAINTEASERLateralThinking2023}, evaluating \acp{LLM} on the full dataset.

\paragraph{Models}
The models used in this experiment are open-source or open-weight, meaning their source code and/or model weights are publicly available. The following models or model families were evaluated: \ac{LLaMA} 3.1 and 3.2~\cite{grattafioriLlama3Herd2024}, \acs{Phi} 3.5 and 4~\cite{abdinPhi3TechnicalReport2024, abdinPhi4TechnicalReport2024}, \acs{Qwen}~\cite{qwenQwen25TechnicalReport2025}, \acs{Gemma}~\cite{teamGemma2Improving2024}, and \acs{Mistral}~\cite{MistralNeMoMistral}. These models represent some of the most advanced and accessible options at the time of writing and are particularly well-suited for reasoning tasks, demonstrating strong capabilities in problem-solving tasks~\cite{grattafioriLlama3Herd2024, abdinPhi3TechnicalReport2024, abdinPhi4TechnicalReport2024, qwenQwen25TechnicalReport2025, teamGemma2Improving2024, MistralNeMoMistral}.

Each model was evaluated across all available parameter sizes fitting within the 24 GB \ac{VRAM} of a single NVIDIA GeForce RTX 3090. For smaller models, FP16 precision was used, as smaller parameter count models tend to be more sensitive to quantization. For larger models, Q8\_0 precision was used when applicable, as recent research indicates no significant performance difference between FP16 and Q8\_0 for large models~\cite{raubaQuantifyingPerturbationImpacts2024, liEvaluatingQuantizedLarge2024}. Q4\_K\_M precision was employed only when VRAM limits required it, as it may cause more noticeable performance degradation.

\paragraph{Evaluation Metrics}
This experiment uses the identical evaluation metrics from the study by \citeauthor{jiangBRAINTEASERLateralThinking2023}, as outlined in \citewithtitle{jiangBRAINTEASERLateralThinking2023}. SemEval also employed the same metrics for evaluation. These include \textbf{Instance-based Accuracy}, which measures the model's performance on each individual puzzle, and \textbf{Group-based Accuracy}, which evaluates the model's consistency across the original puzzle and its variations. For Group-based Accuracy, the model receives a score of 1 only if it correctly solves all three variations of a given puzzle; otherwise, the score is 0. Finally, \textbf{Overall Accuracy} reflects the accuracy across all instances~\cite{jiangBRAINTEASERLateralThinking2023}.

In addition, each metric is calculated for both raw and processed model responses. A \textbf{raw response} refers to the model's direct output, while a \textbf{processed response} involves post-processing to ensure consistent formatting, especially when the model's answer deviates from the expected format. For instance, if the model outputs a full phrase (e.g., \textit{"The best answer for this puzzle is **(A)** The man is a barber"}), the preprocessing step will extract the answer choice "A" and return it as the final response.

\section{Experiment Setup}

This section describes the experimental setup used for each

\section{Evaluation and Results}
