This section outlines the task setup, including the task definition, dataset, and models used in the experiments. It begins with an introduction to the task and dataset, followed by a detailed description of the models employed. The section then emphasizes the experiments conducted, concluding with an analysis of the results obtained.

\subsection{Task Overview}

\paragraph{Task Definition}
In this experiment, the task setup follows the \textit{SemEval 2024 Task 9: Brainteaser}~\cite{jiangBRAINTEASERLateralThinking2023} challenge, simulating a participant's approach, but with the exclusive use of \acp{oLLM}, in contrast to the original competition, where the majority of participants (90\%) relied on closed-source models, e.g., \acs{GPT}-4~\cite{openaiGPT4TechnicalReport2024}, \acs{GPT}-3.5, and Gemini Pro~\cite{teamGeminiFamilyHighly2024}. As in the official task, evaluation is conducted in a direct question-answering format, where each model is given a puzzle question and expected to generate the correct answer. To improve model performance, prompt optimization techniques such as \ac{ICL}~\cite{brownLanguageModelsAre2020} are applied, along with strategies like instructing the model to "think outside the box" or "think step by step". No fine-tuning is performed, with the focus instead on leveraging such techniques to improve results.

\paragraph{Dataset}
While SemEval 2024 Task 9 provides a split dataset, consisting of separate training and evaluation sets, the authors of the original \citetitle{jiangBRAINTEASERLateralThinking2023} benchmarked LLMs on the entire dataset. As the complete dataset is available through their \href{https://github.com/1171-jpg/BrainTeaser}{GitHub repository}, this experiment uses the full dataset, consistent with the original benchmark~\cite{jiangBRAINTEASERLateralThinking2023}. The dataset comprises 1,100 lateral thinking puzzles, structured as question-answer pairs. These puzzles are categorized into two distinct types: \textit{Sentence Puzzles}, which require narrative-based reasoning, and \textit{Word Puzzles}, which involve lexical manipulation tasks~\cite{jiangBRAINTEASERLateralThinking2023}. Furthermore, the puzzles are organized into sets that include original questions as well as their semantic and contextual variations. This structure allows for a systematic assessment of the models' ability to maintain consistent reasoning across different puzzle formulations~\cite{jiangBRAINTEASERLateralThinking2023}.

\paragraph{Models}
The models used in this experiment are open-source or open-weight models, meaning both the source code and model weights are publicly available. The following model or model families were evaluated: \ac{LLaMA} 3.1 and 3.2~\cite{grattafioriLlama3Herd2024}, \acs{Phi} 3.5 and 4~\cite{abdinPhi3TechnicalReport2024, abdinPhi4TechnicalReport2024}, \acs{Qwen}~\cite{qwenQwen25TechnicalReport2025}, \acs{Gemma}~\cite{teamGemma2Improving2024}, and \acs{Mistral}~\cite{MistralNeMoMistral}. For each model, various model sizes were tested, depending on available VRAM (up to 24 GB), either in FP16 or quantized formats (Q8\_0 or Q4\_K\_M).

\paragraph{Evaluation Metrics}
The evaluation metrics in this experiment follow those outlined in \citewithtitle{jiangBRAINTEASERLateralThinking2023}: \textbf{Instance-based Accuracy}, which measures the model's performance on each individual riddle, and \textbf{Group-based Accuracy}, which evaluates the model’s consistency across the original puzzle and its variants. For Group-based Accuracy, the model scores 1 only if it correctly solves all three variations of a given puzzle; otherwise, the score is 0. Finally, \textbf{Overall Accuracy} represents the accuracy across all instances~\cite{jiangBRAINTEASERLateralThinking2023}.

Additionally, each metric was calculated for both raw and processed model responses. A \textbf{raw response} refers to the model's direct output, while a \textbf{processed response} involves post-processing to ensure consistent formatting, especially when the model’s answer deviates from the expected format. For instance, if the model outputs a full phrase (e.g., \textit{"The best answer for this riddle is **(A)** The man is a barber"}) instead of the expected letter choice, the response is processed to extract the correct answer, \textit{A}. This method ensures consistency across various response formats.
