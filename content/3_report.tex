This section outlines the experimental setup, covering the task definition, dataset, and models used. It begins with an introduction to the task and dataset, followed by a detailed description of the models employed. The section then discusses the conducted experiments and concludes with an analysis of the obtained results.

\subsection{Overview: Task, Dataset, and Models}

\paragraph{Task Definition.}
The task setup follows the \textit{SemEval 2024 Task 9: Brainteaser}~\cite{jiangBRAINTEASERLateralThinking2023} challenge, simulating a participant's approach but with the constraint of using \acp{oLLM}. This contrasts with the original competition, where most participants (90\%) relied on closed-source models, such as \acs{GPT}-4~\cite{openaiGPT4TechnicalReport2024}, \acs{GPT}-3.5, and Gemini Pro~\cite{teamGeminiFamilyHighly2024}. As in the official task, evaluation follows a direct question-answering format: each model receives a puzzle question and must generate the correct answer from a provided list. To improve model performance, prompt optimization techniques such as \ac{ICL}~\cite{brownLanguageModelsAre2020} are applied, including additional strategies like instructing the model to \textit{"think outside the box"} or \textit{"think step by step"}. No fine-tuning is performed, instead, the focus is on leveraging prompting techniques.

\paragraph{Dataset.}
The dataset used in this study is the \citewithtitle{jiangBRAINTEASERLateralThinking2023} dataset, publicly available on \hreffootnote{https://github.com/1171-jpg/BrainTeaser}{GitHub}. It consists of 1,100 lateral thinking puzzles formatted as question-answer pairs. The puzzles are categorized into \textit{Sentence Puzzles}, which challenge commonsense expectations based on sentence structures, and \textit{Word Puzzles}, where the answer defies the default meaning of a word and instead focuses on its letter composition~\cite{jiangBRAINTEASERLateralThinking2023}. Additionally, the puzzles are grouped into sets that include the original questions and their semantic and contextual variations, allowing for a systematic assessment of model consistency across different formulations~\cite{jiangBRAINTEASERLateralThinking2023}.

While \textit{SemEval 2024 Task 9} provides an alternative dataset with a predefined training-evaluation split, this experiment follows the approach of the original study~\cite{jiangBRAINTEASERLateralThinking2023}, evaluating \acp{LLM} on the full dataset.

\paragraph{Models.}
Only open-source or open-weight models were considered, ensuring public access to their source code and/or model weights. Based on these criteria, the following models were selected: \ac{LLaMA} 3.1 and 3.2~\cite{grattafioriLlama3Herd2024}, \acs{Phi} 3.5 and 4~\cite{abdinPhi3TechnicalReport2024, abdinPhi4TechnicalReport2024}, \acs{Qwen}~\cite{qwenQwen25TechnicalReport2025}, \acs{Gemma}~\cite{teamGemma2Improving2024}, and \acs{Mistral}~\cite{MistralNeMoMistral}. These models represent the state of the art in reasoning and problem-solving tasks~\cite{grattafioriLlama3Herd2024, abdinPhi3TechnicalReport2024, abdinPhi4TechnicalReport2024, qwenQwen25TechnicalReport2025, teamGemma2Improving2024, MistralNeMoMistral}.

Each model was evaluated across all available parameter sizes that fit within the 24 GB \ac{VRAM} of an NVIDIA GeForce RTX 3090. For smaller models (fewer than 3B parameters), FP16 precision was used, as these models are more sensitive to quantization, which involves representing model weights, activations, and the KV cache in lower-precision formats~\cite{liEvaluatingQuantizedLarge2024}. For larger models (up to 14B parameters), Q8\_0 precision was applied where possible, as recent studies suggest no significant performance difference between FP16 and Q8\_0~\cite{raubaQuantifyingPerturbationImpacts2024, liEvaluatingQuantizedLarge2024}. Models exceeding 27B parameters were evaluated using Q4\_K\_M precision to fit within \ac{VRAM}, though this level of quantization may lead to performance degradation.

\paragraph{Evaluation Metrics.}
Following the official settings by \textcite{jiangBRAINTEASERLateralThinking2023}, model performance was assessed using three accuracy metrics: \textit{(1) Instance-based Accuracy}, which measures the model's performance on each individual puzzle; \textit{(2) Group-based Accuracy}, which evaluates consistency across the original puzzle and its variations (where a model scores 1 only if it correctly solves all three variations of a puzzle, otherwise scoring 0); and \textit{(3) Overall Accuracy}, which reflects accuracy across all instances~\cite{jiangBRAINTEASERLateralThinking2023}.

Each metric is reported for both raw and processed model responses. A \textit{raw response} refers to the model's direct output, whereas a \textit{processed response} involves post-processing to ensure consistent formatting, particularly when the model's answer deviates from the expected format. For example, if the model outputs a full phrase (e.g., \textit{"The best answer for this puzzle is **(A)** The man is a barber"}), the processing step extracts the answer choice "A" as the final response.

\subsection{Experiments}

In all experiments, an enhanced version of the prompt used by \textcite{jiangBRAINTEASERLateralThinking2023} was employed. Specifically, the word \textit{brain teaser} was replaced with \textit{question}. Additionally, the option \textit{"none of the above"} was enclosed in quotes to ensure the model recognizes it as a selectable choice rather than an alternative option.

% \begin{lstlisting}[xleftmargin=0pt, breaklines=true]
% Please select the best answer for the question. Each question has only one correct answer, including the choice 'None of above'. Your answer should only include the choice:

% Question: {question}
% Choice:
% (A) {choice_A}
% (B) {choice_B}
% (C) {choice_C}
% (D) None of the above
% Answer:
% \end{lstlisting}

\paragraph{Zero-Shot Baseline.}
In this experiment, each \ac{LLM} was provided with the default system prompt: \textit{"You are a helpful AI assistant"}. The system prompt serves as an introductory message to guide the behavior of the model. It sets the context for how the model should approach the task and what type of responses are expected. No additional instructions or examples were provided to the model, making this a true zero-shot setting where the model had to rely solely on its pre-trained knowledge to generate answers.

\paragraph{Zero-Shot with Prompt Engineering.}
In this experiment, the same system prompt as the Zero-Shot Baseline was used, with the addition of an instruction for the models to \textit{"\{...\} think step by step \{...\}"} before responding. This technique, a form of prompt engineering, encourages models to break down their reasoning process, potentially leading to more detailed and accurate answers~\cite{weiChainofThoughtPromptingElicits2023}. Other variations of this instruction were also explored, such as prompting the models with \textit{"You are \{...\}. Think step by step to consider hidden meanings and metaphorical interpretations \{...\}"} or \textit{"\{...\} approach each brain teaser from multiple perspectives \{...\}."} These variations were designed to foster creative and multi-faceted reasoning.

\paragraph{Few-Shot.}
For the Few-Shot experiment, the dataset was divided into an evaluation set and a selection pool of examples. Instead of injecting the examples directly into the prompt, the model's API was accessed directly. Each example was provided as user input, followed by the model's answer, which included the correct choice letter. This exchange was then incorporated into the chat history, effectively providing the model with context and helping it learn from the provided examples. This method was chosen because it follows the suggested format by LangChain for effective few-shot learning. The selection of examples was randomized to ensure diversity in answers and prevent the model from learning biases towards a particular answer choice (e.g., always selecting answer "A"). This approach allowed the model to generalize better from a limited number of examples.

\paragraph{Few-Shot with Prompt Engineering.}
In this experiment, the Few-Shot approach was combined with the same prompt engineering techniques as the Zero-Shot with Prompt Engineering experiment. The system prompt was aligned with those used previously, encouraging the models to "think step by step" and consider various formulations. As in the few-shot experiment, each example was injected into the model's chat history as user input followed by the correct answer choice, ensuring that the model had access to a small set of labeled examples while also benefiting from the structured reasoning approach. This setup aimed to combine the benefits of both few-shot learning and prompt engineering to improve the modelâ€™s accuracy and consistency.

\subsection{Evaluation and Results}
