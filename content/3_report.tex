This section outlines the experimental setup, covering the task definition, dataset, and models used. It begins with an introduction to the task and dataset, followed by a detailed description of the models employed. The section then discusses the conducted experiments and concludes with an analysis of the obtained results.

\subsection{Overview: Task, Dataset, and Models}

\paragraph{Task Definition.}
The task setup follows the \textit{SemEval 2024 Task 9: Brainteaser}~\cite{jiangBRAINTEASERLateralThinking2023} challenge, simulating a participant's approach but with the constraint of using \acp{oLLM}. This contrasts with the original competition, where most participants (90\%) relied on closed-source models, such as \acs{GPT}-4~\cite{openaiGPT4TechnicalReport2024}, \acs{GPT}-3.5, and Gemini Pro~\cite{teamGeminiFamilyHighly2024}. As in the official task, evaluation follows a direct question-answering format: each model receives a puzzle question and must generate the correct answer from a provided list. To improve model performance, prompt optimization techniques such as \ac{ICL}~\cite{brownLanguageModelsAre2020} are applied, including additional strategies like instructing the model to \textit{"think outside the box"} or \textit{"think step by step"}. No fine-tuning is performed, instead, the focus is on leveraging prompting techniques.

\paragraph{Dataset.}
Experiments are conducted on the dataset from \citewithtitle{jiangBRAINTEASERLateralThinking2023}, publicly available on \hreffootnote{https://github.com/1171-jpg/BrainTeaser}{GitHub}. It consists of 1,100 lateral thinking puzzles formatted as question-answer pairs. The puzzles are categorized into \textit{Sentence Puzzles}, which challenge commonsense expectations based on sentence structures, and \textit{Word Puzzles}, where the answer defies the default meaning of a word and instead focuses on its letter composition~\cite{jiangBRAINTEASERLateralThinking2023}. Additionally, the puzzles are grouped into sets that include the original questions and their semantic and contextual variations, allowing for a systematic assessment of model consistency across different formulations~\cite{jiangBRAINTEASERLateralThinking2023}.

While \textit{SemEval 2024 Task 9} offers an alternative dataset with a predefined training-evaluation split, this experiment follows the original study~\cite{jiangBRAINTEASERLateralThinking2023}, evaluating \acp{LLM} on the full dataset.

\paragraph{Models.}
The following open-source and open-weight models were selected: \ac{LLaMA} 3.1 and 3.2~\cite{grattafioriLlama3Herd2024}, \acs{Phi} 3.5 and 4~\cite{abdinPhi3TechnicalReport2024, abdinPhi4TechnicalReport2024}, \acs{Qwen}~\cite{qwenQwen25TechnicalReport2025}, \acs{Gemma}~\cite{teamGemma2Improving2024}, and \acs{Mistral}~\cite{MistralNeMoMistral}, as they represent the state of the art in reasoning and problem-solving tasks~\cite{grattafioriLlama3Herd2024, abdinPhi3TechnicalReport2024, abdinPhi4TechnicalReport2024, qwenQwen25TechnicalReport2025, teamGemma2Improving2024, MistralNeMoMistral}.

These models were chosen based on their open-source or open-weight nature, ensuring public access to their source code and/or model weights. Each model was evaluated across all available parameter sizes that fit within the 24 GB \ac{VRAM} of an NVIDIA GeForce RTX 3090. For smaller models (fewer than 3B parameters), FP16 precision was used due to their sensitivity to quantization, which reduces the precision of weights and activations~\cite{liEvaluatingQuantizedLarge2024}. For models up to 14B parameters, Q8\_0 precision was applied where possible, as recent studies suggest no significant performance difference between FP16 and Q8\_0~\cite{raubaQuantifyingPerturbationImpacts2024, liEvaluatingQuantizedLarge2024}. Models with over 27B parameters were evaluated using Q4\_K\_M precision to fit within \ac{VRAM}.

\paragraph{Evaluation Metrics.}
\label{evaluation-metrics}
Model performance was assessed using three accuracy metrics from \textcite{jiangBRAINTEASERLateralThinking2023}: \textit{(1) Instance-based Accuracy}, which measures performance on individual puzzles; \textit{(2) Group-based Accuracy}, which evaluates consistency across the original puzzle and its variations, scoring 1 only if all variations are correctly solved; and \textit{(3) Overall Accuracy}, which reflects accuracy across all instances.

Metrics are reported for both \textit{raw} and \textit{processed} model responses. A \textit{raw response} refers to the model's direct output, where the answer is checked to see if it matches the expected choice or text. A \textit{processed response} involves applying advanced techniques to extract the answer, particularly in cases where the model includes additional text or prefixes, such as \textit{"The best choise is (A)"}.

\subsection{Experiments}

In all experiments, an enhanced version of the prompt from \textcite{jiangBRAINTEASERLateralThinking2023} was used. Specifically, the term \textit{brain teaser} was replaced with \textit{question} to align with standard terminology, and the option \textit{"none of the above"} was enclosed in quotes to ensure the model recognizes it as a selectable choice.

\paragraph{Zero-Shot Baseline.}
\label{zero-shot-prompt}
Each \ac{LLM} is evaluated in a zero-shot setting using the default system prompt: \textit{"You are a helpful AI assistant"}. A system prompt is a general instruction given to the model before the actual user prompt to guide its behavior. In this case, the prompt serves as a primer, offering basic guidance to the model without additional instructions or examples. As a result, the model relies solely on its pre-trained knowledge to generate answers.

\paragraph{Zero-Shot with Prompt Engineering.}
\label{zero-shot-prompt-engineering}
Building on the \nameref{zero-shot-prompt} experiment, 13 different system prompts are tested, each designed to encourage lateral thinking. These prompts include instructions such as \textit{"You are {...}. Think step by step {...}"} or \textit{"{...} consider hidden meanings and metaphorical interpretations {...}"}. Each system prompt ($s_i$) is applied to every model ($m_i$) on a 10\% random split of the dataset. The best-performing prompt for each model is then selected. This system prompt is prefixed to the user prompt, and the model is evaluated on the complete dataset.

\paragraph{Few-Shot Prompting}
\label{few-shot-prompt}

This experiment utilizes \ac{ICL}~\cite{brownLanguageModelsAre2020}, i.e., $n$-few-shot prompting, to improve model performance. Few-shot prompting involves including a small number of examples within the prompt to guide the model's response~\cite{brownLanguageModelsAre2020}. For chat-based models, examples are predominantly provided in the form of a chat history ($H = (u_1, a_1), (u_2, a_2), \ldots, (u_n, a_n), (u_p, a_p)$)~\cite{HowUseFew}, where each example consists of a user prompt ($u_i$) and the corresponding answer ($a_i$). The user query ($u_p$) is inserted as the last item in $H$. This $H$ is then given to the model as input.

To reduce bias, such as the model favoring a particular answer choice due to its frequency in $H$, examples are randomly selected while maintaining diversity in answer labels where possible. The optimal number of examples ($n_i$) for each model ($m_i$) is determined by testing $n \in [1, 8]$ on a random 10\% subset of the data, as running all values on the full dataset is too costly and time-intensive. The results of this initial experiment guide the selection of a unique $n_i$ for each model, and each model is prompted with its respective $n_i$ on the full dataset.

\paragraph{Few-Shot with Prompt Engineering.}
This experiment combines \nameref{zero-shot-prompt-engineering} and \nameref{few-shot-prompt}. As in the \nameref{few-shot-prompt} experiment, examples are presented in a conversational format, integrating both user queries and correct responses before the user query, with the only change being the use of the best-performing system prompt ($s_i$) obtained in \nameref{zero-shot-prompt-engineering}. Each model ($m_i$) is prompted with the optimal number of examples ($n_i$) and the best system prompt ($s_i$) obtained from the zero-shot experiments.

\subsection{Evaluation and Results}

\subsubsection{Zero-Shot Baseline}
This experiment presents the baseline evaluation results, assessing model performance in a zero-shot setting without any prompt optimization or task-specific tuning. This initial experiment establishes how well the models perform naturally without any additional instructions. The results are depicted in \Cref{tab:baseline-zero-shot-evaluation}.

The findings indicate significant performance variations across models and tasks. Larger models, such as \textit{Gemma2:9b-q8-0} and \textit{Gemma2:27b-q4-K-M}, achieve the highest accuracy in the Sentence Puzzle task, with \textit{67.78\%} and \textit{59.81\%}, respectively. Similarly, in the Word Puzzle task, \textit{Gemma2:9b-q8-0} reaches \textit{46.75\%}, outperforming most other models. These results suggest that larger models exhibit superior zero-shot reasoning capabilities.

An interesting observation is that \textit{Qwen2.5:7b-q8-0} outperforms \textit{Qwen2.5:14b-q8-0} in both tasks, indicating that increased model size does not always correlate with improved performance within the \textit{Qwen2.5} family. Additionally, \textit{Mistral-Nemo:12b-2407-q8-0} performs comparably to significantly smaller models, highlighting the influence of architectural differences beyond parameter count.

Smaller models, such as \textit{Llama3.2:1b-fp16} and \textit{Phi3.5:3.8b-mini-fp16}, consistently underperform, often failing to exceed the random baseline. This result underscores their limited reasoning capacity in complex puzzle-solving tasks without explicit adaptation.

In summary, while larger models generally demonstrate stronger performance in a zero-shot setting, the results highlight that model architecture and scaling strategies significantly impact accuracy. Further experiments incorporating prompt optimization and fine-tuning are expected to provide deeper insights into these patterns.

\subsubsection{Zero-Shot with Prompt Engineering}
To further investigate the effect of prompt engineering, this section presents the results obtained by applying various prompt optimization strategies. The results, depicted in \Cref{tab:optimized-zero-shot-evaluation}, demonstrate the extent to which structured prompting can enhance model performance in a zero-shot setting.

Interestingly, while prompt engineering generally improves overall accuracy after postprocessing, the raw results of some models actually decrease. This phenomenon arises because structured prompts often instruct models to \textit{think step by step} or \textit{analyze the problem from different perspectives}. However, models frequently disregard directives to respond with only the final answer and instead generate auxiliary text, such as intermediate reasoning or explanations. As a result, raw accuracy scores tend to drop, even though the extracted final answers, after postprocessing, lead to significant performance gains.

Notably, \textit{Gemma2:9b-q8-0} benefits substantially from prompt optimization, achieving a postprocessed accuracy of \textit{79.43\%} (inside the parentheses) compared to its raw score of \textit{66.99\%}. Similarly, \textit{Qwen2.5:32b-q4-K-M} demonstrates a strong increase, reaching \textit{71.61\%} after postprocessing, up from \textit{65.55\%}. Conversely, some smaller models, such as \textit{Llama3.2:1b-fp16} and \textit{Phi3.5:3.8b-mini-fp16}, continue to struggle, showing minimal or no improvements despite optimized prompting.

These findings highlight the dual-edged nature of prompt engineering: while it enhances reasoning and structured thinking, it also introduces challenges related to response formatting. Future research should explore adaptive decoding techniques or fine-tuned models that better adhere to output constraints while leveraging structured prompts effectively.

\subsubsection{Few-Shot}

\subsubsection{Few-Shot with Prompt Engineering}
