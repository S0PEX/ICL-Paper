This section outlines the experimental setup, covering the task definition, dataset, and models used. It begins with an introduction to the task and dataset, followed by a detailed description of the models employed. The section then discusses the conducted experiments and concludes with an analysis of the obtained results.

\subsection{Overview: Task, Dataset, and Models}

\paragraph{Task Definition.}
The task setup follows the \textit{SemEval 2024 Task 9: Brainteaser}~\cite{jiangBRAINTEASERLateralThinking2023} challenge, simulating a participant's approach but with the constraint of using \acp{oLLM}. This contrasts with the original competition, where most participants (90\%) relied on closed-source models, such as \acs{GPT}-4~\cite{openaiGPT4TechnicalReport2024}, \acs{GPT}-3.5, and Gemini Pro~\cite{teamGeminiFamilyHighly2024}. As in the official task, evaluation follows a direct question-answering format: each model receives a puzzle question and must generate the correct answer from a provided list. To improve model performance, prompt optimization techniques such as \ac{ICL}~\cite{brownLanguageModelsAre2020} are applied, including additional strategies like instructing the model to \textit{"think outside the box"} or \textit{"think step by step"}. No fine-tuning is performed, instead, the focus is on leveraging prompting techniques.

\paragraph{Dataset.}
Experiments are conducted on the dataset from \citewithtitle{jiangBRAINTEASERLateralThinking2023}, publicly available on \hreffootnote{https://github.com/1171-jpg/BrainTeaser}{GitHub}. It consists of 1,100 lateral thinking puzzles formatted as question-answer pairs. The puzzles are categorized into \textit{Sentence Puzzles}, which challenge commonsense expectations based on sentence structures, and \textit{Word Puzzles}, where the answer defies the default meaning of a word and instead focuses on its letter composition~\cite{jiangBRAINTEASERLateralThinking2023}. Additionally, the puzzles are grouped into sets that include the original questions and their semantic and contextual variations, allowing for a systematic assessment of model consistency across different formulations~\cite{jiangBRAINTEASERLateralThinking2023}.

While \textit{SemEval 2024 Task 9} offers an alternative dataset with a predefined training-evaluation split, this experiment follows the original study~\cite{jiangBRAINTEASERLateralThinking2023}, evaluating \acp{LLM} on the full dataset.

\paragraph{Models.}
The following open-source and open-weight models were selected: \ac{LLaMA} 3.1 and 3.2~\cite{grattafioriLlama3Herd2024}, \acs{Phi} 3.5 and 4~\cite{abdinPhi3TechnicalReport2024, abdinPhi4TechnicalReport2024}, \acs{Qwen}~\cite{qwenQwen25TechnicalReport2025}, \acs{Gemma}~\cite{teamGemma2Improving2024}, and \acs{Mistral}~\cite{MistralNeMoMistral}, as they represent the state of the art in reasoning and problem-solving tasks~\cite{grattafioriLlama3Herd2024, abdinPhi3TechnicalReport2024, abdinPhi4TechnicalReport2024, qwenQwen25TechnicalReport2025, teamGemma2Improving2024, MistralNeMoMistral}.

These models were chosen based on their open-source or open-weight nature, ensuring public access to their source code and/or model weights. Each model was evaluated across all available parameter sizes that fit within the 24 GB \ac{VRAM} of an NVIDIA GeForce RTX 3090. For smaller models (fewer than 3B parameters), FP16 precision was used due to their sensitivity to quantization, which reduces the precision of weights and activations~\cite{liEvaluatingQuantizedLarge2024}. For models up to 14B parameters, Q8\_0 precision was applied where possible, as recent studies suggest no significant performance difference between FP16 and Q8\_0~\cite{raubaQuantifyingPerturbationImpacts2024, liEvaluatingQuantizedLarge2024}. Models with over 27B parameters were evaluated using Q4\_K\_M precision to fit within \ac{VRAM}.

\paragraph{Evaluation Metrics.}
\label{evaluation-metrics}
Model performance was assessed using three accuracy metrics from \textcite{jiangBRAINTEASERLateralThinking2023}: \textit{(1) Instance-based Accuracy}, which measures performance on individual puzzles; \textit{(2) Group-based Accuracy}, which evaluates consistency across the original puzzle and its variations, scoring 1 only if all variations are correctly solved; and \textit{(3) Overall Accuracy}, which reflects accuracy across all instances.

Metrics are reported for both \textit{raw} and \textit{processed} model responses. A \textit{raw response} refers to the model's direct output, where the answer is checked to see if it matches the expected choice or text. A \textit{processed response} involves applying advanced techniques to extract the answer, particularly in cases where the model includes additional text or prefixes, such as \textit{"The best choise is (A)"}.

\subsection{Experiments}

In all experiments, an enhanced version of the prompt from \textcite{jiangBRAINTEASERLateralThinking2023} was used. Specifically, the term \textit{"brain teaser"} was replaced with \textit{"question"} to align with standard terminology, and the option \textit{"none of the above"} was enclosed in quotes to ensure the model recognizes it as a selectable choice.

\paragraph{Zero-Shot Baseline.}
\label{sec:zero-shot-prompt}
Each \ac{LLM} is evaluated in a zero-shot setting using the default system prompt: \textit{"You are a helpful AI assistant"}. A system prompt is a general instruction given to the model before the actual user prompt to guide its behavior. In this case, the prompt serves as a primer, offering basic guidance to the model without additional instructions or examples. As a result, the model relies solely on its pre-trained knowledge to generate answers.

\paragraph{Zero-Shot with Prompt Engineering.}
\label{sec:zero-shot-prompt-engineering}
Building on the \nameref{sec:zero-shot-prompt} experiment, 13 different system prompts are tested, each designed to encourage lateral thinking. These prompts include instructions such as \textit{"You are \{...\}. Think step by step \{...\}"} or \textit{"\{...\} consider hidden meanings and metaphorical interpretations {...}"}. Each system prompt ($s_i$) is applied to every model ($m_i$) on a 10\% random split of the dataset. The best-performing prompt for each model is then selected. This system prompt is prefixed to the user prompt, and the model is evaluated on the complete dataset.

\paragraph{Few-Shot Prompting}
\label{sec:few-shot-prompt}

This experiment utilizes \ac{ICL}~\cite{brownLanguageModelsAre2020}, i.e., $n$-few-shot prompting, to improve model performance. Few-shot prompting involves presenting a constrained set of examples within the prompt to aid \acl{ICL} and guide the model's response generation~\cite{brownLanguageModelsAre2020}. For chat-based models, examples are predominantly provided in a conversational form, i.e., a chat history ($H = ((u_1, a_1), (u_2, a_2), \ldots, (u_n, a_n), u_p)$)~\cite{HowUseFew}, where each example consists of a user prompt ($u_i$) and the corresponding answer ($a_i$). The user query ($u_p$) is inserted as the last item in $H$. This $H$ is then given to the model as input.

To reduce bias, such as the model favoring a particular answer choice due to its frequency in $H$, examples are randomly selected while maintaining diversity in answer labels where possible. The optimal number of examples ($n_i$) for each model ($m_i$) is determined by testing $n \in [1, 8]$ on a random 10\% subset of the data, as running all values on the full dataset is too costly and time-intensive. The results of this initial experiment guide the selection of a unique $n_i$ for each model, and each model is prompted with its respective $n_i$ on the full dataset.

\paragraph{Few-Shot with Prompt Engineering.}
\label{sec:few-shot-optimized}

This experiment combines \nameref{sec:zero-shot-prompt-engineering} and \nameref{sec:few-shot-prompt}. As in the \nameref{sec:few-shot-prompt} experiment, examples are presented in a conversational format, integrating both user queries and correct responses before the user query, with the only change being the use of the best-performing system prompt ($s_i$) obtained in \nameref{sec:zero-shot-prompt-engineering}. Each model ($m_i$) is prompted with the optimal number of examples ($n_i$) and the best system prompt ($s_i$) obtained from the zero-shot experiments.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                        Results                          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Results}

The results address three main questions: (1) the raw performance of \ac{oLLM} across all four experimental settings, (2) the impact of postprocessing on performance, and (3) how the best-performing model compares to other predominantly closed \acp{LLM} in the \textit{SemEval}~\cite{jiangSemEval2024Task92024} competition.

Before discussing the results, it is essential to define the terms \textit{baseline} and \textit{optimized} results. \textit{Baseline} refers to the initial results obtained using the default system prompt, while \textit{optimized} refers to the experiments with system prompt engineering applied. Additionally, to ensure consistency with data handling in the competition, the evaluation considers only \textit{raw} model responses, as \citeauthor{jiangBRAINTEASERLateralThinking2023}~\cite{jiangBRAINTEASERLateralThinking2023,jiangSemEval2024Task92024} computed performance based on raw model responses. While relying on raw responses is valid, given that models are explicitly instructed to respond in a specific format, careful interpretation of the results is necessary.

Raw performance, i.e., performance based on raw responses, alone does not fully reflect the capabilities of \acp{LLM} due to \textit{output format biases}. This refers to the tendency of models to inconsistently interpret or disregard prompt instructions, such as \textit{"Your answer should only include the choice"}. As a result, models may generate correct answers but include additional explanatory text, which can lead to incorrect classifications. Thus, raw performance primarily indicates a model's ability to follow the required response format and produce an answer simultaneously, rather than its actual problem-solving capabilities. This distinction is further explored in \Cref{par:raw-vs-post-performance}.

\subsubsection{Performance across Experiments}
\label{sec:performance-across-experiments}

The main results of all experiments are summarized in \Cref{tab:baseline-vs-optimized-zero-shot-raw,tab:baseline-zero-shot-vs-baseline-few-shot-raw,tab:baseline-vs-optimized-few-shot-raw}. To gain insights into how the models performed and how \ac{ICL} and prompt engineering influenced the results, the tables are discussed sequentially. Additionally, \Cref{tab:full-zero-shot,tab:full-few-shot,tab:optimized-zero-shot-vs-optimized-few-shot}, containing both raw and postprocessed results, are included but not discussed in detail.

\paragraph{Zero-Shot}

\Cref{tab:baseline-vs-optimized-zero-shot-raw} presents the results from the \nameref{sec:zero-shot-prompt} and \nameref{sec:zero-shot-prompt-engineering} experiments. In the baseline condition, only about half of the models outperform the random guesser (\textit{RANDOM*}). Smaller models, such as LLama 3.2, Phi 3.5, QWEN2.5, and the Gemma2 variant with fewer than 7B parameters, often underperform and fail to surpass the random guesser performance on both \ac{SP} and \ac{WP} subsets. Only four out of 15 models exceed 50\% performance on \ac{SP}, and none achieve more than 50\% on \ac{WP}. The best-performing model, \textit{Gemma2:9B}, achieves 68.10\% and 47.36\% for \ac{SP} and \ac{WP}, respectively.

For the optimized results, after system prompt optimization, model performance systematically decreases by 3.68\% and 5.81\% compared to the baseline, with \textit{QWEN2:32B} being the best-performing model at 65.55\% for \ac{SP} and 54.27\% for \ac{WP}. This decrease is likely due to excessive auxiliary text generated by the models, which negatively affects raw performance. Postprocessing clarifies that the decline is mainly caused by this auxiliary text, as performance increases by 7.15\% and 8.89\% after postprocessing (see \cref{tab:full-zero-shot}). Despite taking postprocessing into account, smaller models still underperform, failing to exceed random guesser performance. Meanwhile, larger models show notable improvements, with some achieving high single-digit or low double-digit performance gains. Notably, \textit{QWEN2:32B} shows significant improvements from prompt engineering, with gains of 16.59\% and 29.88\%, respectively. Interestingly, certain models, i.e., \textit{QWEN2:32B} shows significant improvements from prompt engineering, with gains of 16.59\% and 29.88\%, respectively. However, parameter count doesn't seem to be the most important factor, as \textit{mistral-nemo:12B} (25.36\% and 11.79\%) performed comparably poorly to smaller models and did not match the performance of its equally sized counterparts.

\paragraph{Few-Shot}

\Cref{tab:baseline-zero-shot-vs-baseline-few-shot-raw} presents the results from the \nameref{sec:zero-shot-prompt} and \nameref{sec:few-shot-prompt} experiments. The results show that few-shot prompting significantly enhances model performance, with an average increase of 20.22\% and 23.38\% in overall performance, or 14.56\% and 20.44\% after postprocessing (see \cref{tab:full-few-shot}). Now, nine out of 15 models exceed 50\% performance on \ac{SP}, and four out of 15 models surpass 50\% on \ac{WP}. These improvements are notable as they reflect consistent gains across multiple models, not just a few exceptional ones, indicating that few-shot prompting benefits models overall.

Further analysis reveals that smaller models also see substantial improvements with few-shot prompting. Previously, no smaller models (with fewer than 7B parameters) exceeded random guesser performance (24.04\% and 25.34\%). However, with few-shot prompting, these models now achieve average performances of 31.36\% and 31.72\%, respectively. Larger models also show significant improvements, with the top-performing models now being \textit{Phi4:14B} (77.72\% and 69.53\%) and \textit{Gemma2:27B} (79.81\% and 64.83\%), surpassing the previously best-performing model, \textit{Gemma2:9B}, from the zero-shot setting.

At this stage, the question arises whether few-shot prompting is more effective at eliciting information from the models compared to prompt optimization, or if the observed performance improvements could be primarily due to the models now having a clearer understanding of the expected output format. To investigate this, a manual inspection of model responses was performed, which revealed that, with few-shot prompting, most models now consistently output in the required format, avoiding the auxiliary text present in the zero-shot baseline. This suggests that the lower performance observed in the earlier optimized system prompt zero-shot experiment may have been due to output format inconsistencies rather than a lack of model knowledge. A further evaluation of the \nameref{sec:few-shot-optimized} experiment could provide more clarity on this matter.

\paragraph{Few-Shot with Optimized System Prompt}

\Cref{tab:baseline-vs-optimized-few-shot-raw} compares the results of the baseline and system prompt optimized few-shot experiments. As expected, combining both techniques improves performance, but only for the \ac{SP} set, with a 6.10\% improvement over baseline few-shot. For \ac{WP}, however, there is a 4.20\% decrease in performance.

This aligns with the zero-shot experiment, where system prompt optimization negatively impacted performance. In that case, prompt engineering worsened raw performance, while postprocessing led to significant improvement. The same trend is observed here, where postprocessed results show a 6.27\% improvement for \ac{SP}, while \ac{WP} sees only a marginal increase of 0.05\%. The top-performing models remain \textit{Gemma2:27B} (79.81 and 64.83\%) and \textit{Phi4:14B} (77.72 and 69.53\%).

Although combining few-shot with an optimized system prompt results in a 1.90\% net improvement, this change is too small to definitively categorize the combination as superior to baseline few-shot. The modest increase may be due to run-to-run variations or model instability. Therefore, rerunning the experiment across multiple runs and adjusting parameters such as model seed and temperature could provide greater clarity and enable a more accurate evaluation of the benefits of combining these techniques.

\paragraph{Conclusion}
The results demonstrate that few-shot prompting improves model performance, particularly for larger models, while smaller models continue to face challenges. Specifically, few-shot prompting yields gains of 20.22\% for \acf{SP} and 23.38\% for \acf{WP} in raw performance. System prompt optimization, on the other hand, leads to a decrease of 3.68\% for \ac{SP} and 5.81\% for \ac{WP} due to additional auxiliary text, although postprocessing helps recover some accuracy. When combining few-shot prompting with system prompt optimization, performance improves by 6.10\% for \ac{SP}, but \ac{WP} experiences a 4.20\% decrease. These findings suggest that few-shot prompting alone might be more effective, as the combined approach yields only modest improvements. Overall, \textit{Phi4:14B} (77.72\% and 69.53\%) performed best with system-prompt optimized few-shot prompting.

\subsubsection{Implications of Raw vs. Postprocessed Performance}
\label{par:raw-vs-post-performance}

As highlighted in the previous section, system prompt optimization can negatively impact raw performance despite leading to substantial gains after postprocessing model responses. With insights from \Cref{sec:performance-across-experiments}, this pattern is evident in both zero-shot and few-shot experiments, where prompt engineering initially reduces raw accuracy but improves results once auxiliary text is accounted for.

Raw performance metrics do not consider auxiliary text, which often arises when applying prompt engineering techniques such as instructing the model to \textit{Think Step-By-Step}. This additional text is the primary factor lowering raw scores, even when the actual answers remain correct. Consequently, if one were to rely solely on raw performance, it would appear that prompt engineering, on average, decreases accuracy, while few-shot prompting significantly enhances it. However, this interpretation is not entirely accurate.

When considering postprocessed results, both prompt engineering and few-shot significantly improve performance by 7.15\% and 8.89\%, 14.56\% and 20.44\% respectively. Moreover, combining both techniques leads to improvements in both raw and postprocessed metrics, with gains of approximately 1.9\% in raw performance and 6.33\% in postprocessed performance over baseline few-shot.

These findings underscore the importance of evaluating model outputs beyond raw performance alone. Relying exclusively on raw evaluations can lead to misleading conclusions, as models may generate correct answers but with excessive verbosity. Although a more in-depth analysis was beyond the scope of this paper, the results suggest that developing methods to extract correct responses from verbose outputs would be highly beneficial. This is particularly relevant in applications where correctness takes precedence over strict format adherence.

\subsubsection{Performance Comparison with SemEval Participants}

Comparing the models presented in this paper with those in the SemEval paper\cite{jiangSemEval2024Task92024} is complex and should be done with caution due to differences in experimental setups and the variety of models and techniques employed by different teams. For example, models in the SemEval competition often used advanced techniques such as temperature tuning, identifying challenging training instances, and incorporating methods like \ac{CoT} (Chain-of-Thought) to generate rationales for correct and incorrect answers, which were then used in the prompt.

Despite these differences, we can use the results gathered here to get an impression of how closed and open models performed on the task. The best-performing model in this paper, \textit{Phi4:14B} (77.72\% and 69.53\%) (see \cref{tab:baseline-vs-optimized-few-shot-raw}), would have ranked 18th out of 30 on \ac{SP} and placed around 15th on \ac{WP}. In contrast, closed-source models in the task achieved performance in the high 90s on both \ac{SP} and \ac{WP} (Figure 4 by \textcite{jiangSemEval2024Task92024}). Notably, as observed in this paper, few-shot prompting was also the most effective approach for improving performance in SemEval.

Although some top teams in SemEval employed advanced techniques to enhance their models, it is important to note that certain tested \acp{oLLM}, like \textit{Phi4} and \textit{Gemma2}, achieved competitive performance without relying on these methods. This suggests that strong results can still be achieved through prompt engineering and model architecture alone, without the need for more complex strategies or relying on closed \acp{LLM} in such tasks.