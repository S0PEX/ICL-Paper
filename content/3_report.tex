This section outlines the experimental setup, covering the task definition, dataset, and models used. It begins with an introduction to the task and dataset, followed by a detailed description of the models employed. The section then discusses the conducted experiments and concludes with an analysis of the obtained results.

\subsection{Overview: Task, Dataset, and Models}

\paragraph{Task Definition.}
The task setup follows the \textit{SemEval 2024 Task 9: Brainteaser}~\cite{jiangBRAINTEASERLateralThinking2023} challenge, simulating a participant's approach but with the constraint of using \acp{oLLM}. This contrasts with the original competition, where most participants (90\%) relied on closed-source models, such as \acs{GPT}-4~\cite{openaiGPT4TechnicalReport2024}, \acs{GPT}-3.5, and Gemini Pro~\cite{teamGeminiFamilyHighly2024}. As in the official task, evaluation follows a direct question-answering format: each model receives a puzzle question and must generate the correct answer from a provided list. To improve model performance, prompt optimization techniques such as \ac{ICL}~\cite{brownLanguageModelsAre2020} are applied, including additional strategies like instructing the model to \textit{"think outside the box"} or \textit{"think step by step"}. No fine-tuning is performed, instead, the focus is on leveraging prompting techniques.

\paragraph{Dataset.}
Experiments are conducted on the dataset from \citewithtitle{jiangBRAINTEASERLateralThinking2023}, publicly available on \hreffootnote{https://github.com/1171-jpg/BrainTeaser}{GitHub}. It consists of 1,100 lateral thinking puzzles formatted as question-answer pairs. The puzzles are categorized into \textit{Sentence Puzzles}, which challenge commonsense expectations based on sentence structures, and \textit{Word Puzzles}, where the answer defies the default meaning of a word and instead focuses on its letter composition~\cite{jiangBRAINTEASERLateralThinking2023}. Additionally, the puzzles are grouped into sets that include the original questions and their semantic and contextual variations, allowing for a systematic assessment of model consistency across different formulations~\cite{jiangBRAINTEASERLateralThinking2023}.

While \textit{SemEval 2024 Task 9} offers an alternative dataset with a predefined training-evaluation split, this experiment follows the original study~\cite{jiangBRAINTEASERLateralThinking2023}, evaluating \acp{LLM} on the full dataset.

\paragraph{Models.}
The following open-source and open-weight models were selected: \ac{LLaMA} 3.1 and 3.2~\cite{grattafioriLlama3Herd2024}, \acs{Phi} 3.5 and 4~\cite{abdinPhi3TechnicalReport2024, abdinPhi4TechnicalReport2024}, \acs{Qwen}~\cite{qwenQwen25TechnicalReport2025}, \acs{Gemma}~\cite{teamGemma2Improving2024}, and \acs{Mistral}~\cite{MistralNeMoMistral}, as they represent the state of the art in reasoning and problem-solving tasks~\cite{grattafioriLlama3Herd2024, abdinPhi3TechnicalReport2024, abdinPhi4TechnicalReport2024, qwenQwen25TechnicalReport2025, teamGemma2Improving2024, MistralNeMoMistral}.

These models were chosen based on their open-source or open-weight nature, ensuring public access to their source code and/or model weights. Each model was evaluated across all available parameter sizes that fit within the 24 GB \ac{VRAM} of an NVIDIA GeForce RTX 3090. For smaller models (fewer than 3B parameters), FP16 precision was used due to their sensitivity to quantization, which reduces the precision of weights and activations~\cite{liEvaluatingQuantizedLarge2024}. For models up to 14B parameters, Q8\_0 precision was applied where possible, as recent studies suggest no significant performance difference between FP16 and Q8\_0~\cite{raubaQuantifyingPerturbationImpacts2024, liEvaluatingQuantizedLarge2024}. Models with over 27B parameters were evaluated using Q4\_K\_M precision to fit within \ac{VRAM}.

\paragraph{Evaluation Metrics.}
\label{evaluation-metrics}
Model performance was assessed using three accuracy metrics from \textcite{jiangBRAINTEASERLateralThinking2023}: \textit{(1) Instance-based Accuracy}, which measures performance on individual puzzles; \textit{(2) Group-based Accuracy}, which evaluates consistency across the original puzzle and its variations, scoring 1 only if all variations are correctly solved; and \textit{(3) Overall Accuracy}, which reflects accuracy across all instances.

Metrics are reported for both \textit{raw} and \textit{processed} model responses. A \textit{raw response} refers to the model's direct output, where the answer is checked to see if it matches the expected choice or text. A \textit{processed response} involves applying advanced techniques to extract the answer, particularly in cases where the model includes additional text or prefixes, such as \textit{"The best choise is (A)"}.

\subsection{Experiments}

In all experiments, an enhanced version of the prompt from \textcite{jiangBRAINTEASERLateralThinking2023} was used. Specifically, the term \textit{"brain teaser"} was replaced with \textit{"question"} to align with standard terminology, and the option \textit{"none of the above"} was enclosed in quotes to ensure the model recognizes it as a selectable choice.

\paragraph{Zero-Shot Baseline.}
\label{sec:zero-shot-prompt}
Each \ac{LLM} is evaluated in a zero-shot setting using the default system prompt: \textit{"You are a helpful AI assistant"}. A system prompt is a general instruction given to the model before the actual user prompt to guide its behavior. In this case, the prompt serves as a primer, offering basic guidance to the model without additional instructions or examples. As a result, the model relies solely on its pre-trained knowledge to generate answers.

\paragraph{Zero-Shot with Prompt Engineering.}
\label{sec:zero-shot-prompt-engineering}
Building on the \nameref{sec:zero-shot-prompt} experiment, 13 different system prompts are tested, each designed to encourage lateral thinking. These prompts include instructions such as \textit{"You are \{...\}. Think step by step \{...\}"} or \textit{"\{...\} consider hidden meanings and metaphorical interpretations {...}"}. Each system prompt ($s_i$) is applied to every model ($m_i$) on a 10\% random split of the dataset. The best-performing prompt for each model is then selected. This system prompt is prefixed to the user prompt, and the model is evaluated on the complete dataset.

\paragraph{Few-Shot Prompting}
\label{sec:few-shot-prompt}

This experiment utilizes \ac{ICL}~\cite{brownLanguageModelsAre2020}, i.e., $n$-few-shot prompting, to improve model performance. Few-shot prompting involves presenting a constrained set of examples within the prompt to aid \acl{ICL} and guide the model's response generation~\cite{brownLanguageModelsAre2020}. For chat-based models, examples are predominantly provided in a conversational form, i.e., a chat history ($H = ((u_1, a_1), (u_2, a_2), \ldots, (u_n, a_n), u_p)$)~\cite{HowUseFew}, where each example consists of a user prompt ($u_i$) and the corresponding answer ($a_i$). The user query ($u_p$) is inserted as the last item in $H$. This $H$ is then given to the model as input.

To reduce bias, such as the model favoring a particular answer choice due to its frequency in $H$, examples are randomly selected while maintaining diversity in answer labels where possible. The optimal number of examples ($n_i$) for each model ($m_i$) is determined by testing $n \in [1, 8]$ on a random 10\% subset of the data, as running all values on the full dataset is too costly and time-intensive. The results of this initial experiment guide the selection of a unique $n_i$ for each model, and each model is prompted with its respective $n_i$ on the full dataset.

\paragraph{Few-Shot with Prompt Engineering.}
\label{sec:few-shot-optimized}

This experiment combines \nameref{sec:zero-shot-prompt-engineering} and \nameref{sec:few-shot-prompt}. As in the \nameref{sec:few-shot-prompt} experiment, examples are presented in a conversational format, integrating both user queries and correct responses before the user query, with the only change being the use of the best-performing system prompt ($s_i$) obtained in \nameref{sec:zero-shot-prompt-engineering}. Each model ($m_i$) is prompted with the optimal number of examples ($n_i$) and the best system prompt ($s_i$) obtained from the zero-shot experiments.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                        Results                          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Results}

The results address three key questions: (1) the raw performance of \ac{oLLM} across all experimental settings, (2) the impact of postprocessing on performance, and (3) how the best-performing model compares to other predominantly closed \acp{LLM} in the \textit{SemEval}~\cite{jiangSemEval2024Task92024} competition.

%To clarify, \textit{baseline} refers to results with the default system prompt, while \textit{optimized} refers to results with prompt engineering. The evaluation considers only \textit{raw} model responses to maintain consistency with the competition, where performance was based on raw responses~\cite{jiangBRAINTEASERLateralThinking2023,jiangSemEval2024Task92024}. However, raw performance may not fully reflect a model's capabilities due to \textit{output format biases}, where models may generate correct answers but include extra text, leading to incorrect classifications. Thus, raw performance mainly indicates a model's ability to follow prompt instructions, not its actual problem-solving skills. This is further discussed in \Cref{par:raw-vs-post-performance}.

\subsubsection{Performance across Experiments}
\label{sec:performance-across-experiments}

The main results of all experiments are summarized in \Cref{tab:baseline-vs-optimized-zero-shot-raw,tab:baseline-zero-shot-vs-baseline-few-shot-raw,tab:baseline-vs-optimized-few-shot-raw}. To gain insights into how the models performed and how \ac{ICL} and prompt engineering influenced the results, the tables are discussed sequentially. Additionally, \Cref{tab:full-zero-shot,tab:full-few-shot,tab:optimized-zero-shot-vs-optimized-few-shot}, containing both raw and postprocessed results, are included but not discussed in detail.

\paragraph{Zero-Shot}

\Cref{tab:baseline-vs-optimized-zero-shot-raw} presents the results from the \nameref{sec:zero-shot-prompt} and \nameref{sec:zero-shot-prompt-engineering} experiments. In the baseline condition, only about half of the models outperform the random guesser (\textit{RANDOM*}). Smaller models, such as LLama 3.2, Phi 3.5, QWEN2.5, and the Gemma2 variant with fewer than 7B parameters, often underperform and fail to surpass random guesser performance on both \ac{SP} and \ac{WP} subsets. Only four out of 15 models exceed 50\% performance on \ac{SP}, and none achieve more than 50\% on \ac{WP}. The best performing model, \textit{Gemma2:9B}, achieves 68.10\% and 47.36\% for \ac{SP} and \ac{WP}, respectively.

For the optimized results, after system prompt optimization, model performance systematically decreases by 3.68\% and 5.81\% compared to the baseline, with \textit{QWEN2.5:32B} being the best performing model at 65.55\% for \ac{SP} and 54.27\% for \ac{WP}. This decrease is likely due to excessive auxiliary text generated by the models, which negatively affects raw performance. Postprocessing clarifies that the decline is mainly caused by this auxiliary text, as performance increases by 7.15\% and 8.89\% after postprocessing (see \cref{tab:full-zero-shot}). Despite postprocessing, smaller models still underperform, failing to exceed random guesser performance. Meanwhile, larger models show notable improvements, with some achieving high single digit or low double digit performance gains. Notably, \textit{QWEN2.5:32B} shows the biggest improvements from prompt engineering, with raw gains of 16.59\% and 29.88\%, respectively.

However, there is no clear indicator of when models perform worse with prompt engineering or not. But mainly, the QWEN family with at least 7B parameters seems to benefit significantly from it. The speculation is that this phenomenon must be related to the model's architecture.

\paragraph{Few-Shot}

\Cref{tab:baseline-zero-shot-vs-baseline-few-shot-raw} presents the results from the \nameref{sec:zero-shot-prompt} and \nameref{sec:few-shot-prompt} experiments. The results show that few-shot prompting significantly enhances model performance, with an average increase of 20.22\% and 23.38\% in raw overall performance, or 14.56\% and 20.44\% postprocessing, respectively (see \cref{tab:full-few-shot}). Compared to zero-shot, nine out of 15 models now exceed 50\% performance on \ac{SP}, and four out of 15 models surpass 50\% on \ac{WP}. These improvements are notable as they reflect consistent gains across multiple models, not just a few exceptional ones, indicating that few-shot prompting benefits models overall.

Further analysis reveals that smaller models also see substantial improvements with few-shot prompting. Previously, no smaller models (with fewer than 7B parameters) exceeded random guesser performance (24.04\% and 25.34\%). However, with few-shot prompting, these models now achieve average performances of 31.36\% and 31.72\%, respectively. Larger models also show significant improvements, with the top-performing models now being \textit{Phi4:14B} (77.72\% and 69.53\%) and \textit{Gemma2:27B} (79.81\% and 64.83\%), surpassing the previously best-performing model, \textit{Gemma2:9B}, from the zero-shot setting.

At this stage, the question arises whether few-shot prompting is more effective at eliciting information from the models compared to prompt optimization, or if the observed performance improvements could be primarily due to the models now having a clearer understanding of the expected output format. To investigate this, a manual inspection of model responses was performed, which revealed that, with few-shot prompting, most models now consistently output in the required format, avoiding the auxiliary text present in the zero-shot baseline. This suggests that the lower performance observed in the earlier optimized system prompt zero-shot experiment may have been due to output format inconsistencies rather than a lack of model knowledge. A further evaluation of the \nameref{sec:few-shot-optimized} experiment could provide more clarity on this matter.

\paragraph{Few-Shot with Optimized System Prompt}

\Cref{tab:baseline-vs-optimized-few-shot-raw} compares the results of the baseline and system prompt optimized few-shot experiments. As expected, combining both techniques improves performance, but only for the \ac{SP} set, with a 6.10\% improvement over baseline few-shot. For \ac{WP}, however, there is a 4.20\% decrease in performance.

This aligns with the zero-shot experiment, where system prompt optimization negatively impacted performance. In that case, prompt engineering worsened raw performance, while postprocessing led to significant improvement. The same trend is observed here, where postprocessed results show a 6.27\% improvement for \ac{SP}, while \ac{WP} sees only a marginal increase of 0.05\% (while being negative in the raw results). The top-performing models remain \textit{Gemma2:27B} (79.81\% and 64.83\%) and \textit{Phi4:14B} (77.72\% and 69.53\%).

Although combining few-shot with an optimized system prompt results in a 1.90\% net improvement, this change is too small to definitively categorize the combination as superior to baseline few-shot. The modest increase may be due to run-to-run variations or model instability. Therefore, rerunning the experiment across multiple runs and adjusting parameters such as model seed and temperature could provide greater clarity and enable a more accurate evaluation of the benefits of combining these techniques.

\paragraph{Conclusion}

The results demonstrate that few-shot prompting improves model performance, particularly for larger models, while smaller models continue to face challenges. Specifically, few-shot prompting yields gains of 20.22\% for \acf{SP} and 23.38\% for \acf{WP} in raw performance. System prompt optimization, on the other hand, leads to a decrease of 3.68\% for \ac{SP} and 5.81\% for \ac{WP} due to additional auxiliary text, although postprocessing helps recover some accuracy. When combining few-shot prompting with system prompt optimization, performance improves by 6.10\% for \ac{SP}, but \ac{WP} experiences a 4.20\% decrease. These findings suggest that few-shot prompting alone might be more effective, at least in terms of raw performance, as the combined approach yields only modest improvements. Overall, \textit{Phi4:14B} (77.72\% and 69.53\%) performed best with system-prompt-optimized few-shot prompting.

\subsubsection{Implications of Raw vs. Postprocessed Performance}
\label{par:raw-vs-post-performance}

As highlighted in the previous section, system prompt optimization can negatively impact raw performance despite leading to substantial gains after postprocessing model responses. With insights from \Cref{sec:performance-across-experiments}, this pattern is evident in both zero-shot and few-shot experiments, where prompt engineering initially reduces raw accuracy but improves results once auxiliary text is accounted for.

Raw performance metrics do not consider auxiliary text, which often arises when applying prompt engineering techniques such as instructing the model to \textit{Think Step-By-Step}. This additional text is the primary factor lowering raw scores, even when the actual answers remain correct. Consequently, if one were to rely solely on raw performance, it would appear that prompt engineering, on average, decreases accuracy, while few-shot prompting significantly enhances it. However, this interpretation is not entirely accurate.

When considering postprocessed results, both prompt engineering and few-shot significantly improve performance by 7.15\% and 8.89\%, 14.56\% and 20.44\% respectively. Moreover, combining both techniques leads to improvements in both raw and postprocessed metrics, with gains of approximately 1.9\% in raw performance and 6.33\% in postprocessed performance over baseline few-shot.

These findings underscore the importance of evaluating model outputs beyond raw performance alone. Relying exclusively on raw evaluations can lead to misleading conclusions, as models may generate correct answers but with excessive verbosity. Although a more in-depth analysis was beyond the scope of this paper, the results suggest that developing methods to extract correct responses from verbose outputs would be highly beneficial. This is particularly relevant in applications where correctness takes precedence over strict format adherence.

\subsubsection{Performance Comparison with SemEval Participants}

The best-performing model in this paper, \textit{Phi4:14B} (77.72\% and 69.53\%) with few-shot prompting, would have ranked 18th out of 30 on \ac{SP} and approximately 15th on \ac{WP} on the \href{https://brainteasersem.github.io/#leaderboard}{SemEval leaderboard}. In contrast, closed-source models, such as ChatGPT 4~\cite{openaiGPT4TechnicalReport2024}, achieved high 90s performance on both \ac{SP} and \ac{WP} (Figure 4 by \textcite{jiangSemEval2024Task92024}), but these results were only attained by leveraging more advanced techniques. Many of the top submissions combined several optimization strategies to reach such high scores, with few-shot prompting being a crucial component.

For instance, some teams identified particularly challenging training instances and employed methods like \ac{CoT} (Chain-of-Thought) to generate rationales for both correct and incorrect answers, which were then included in the few-shot examples for the prompt. Other teams adjusted the model's temperature, controlling the level of randomness in the model's responses, which helped to enhance performance.  It's also worth noting that the here tested models are relatively small, in terms of parameters, compared to other open-source models like \textit{LLama 3 (405B)} that can compete with GPT-4~\cite{grattafioriLlama3Herd2024,openaiGPT4TechnicalReport2024}.

Considering these factors, it is reasonable to conclude that certain tested \acp{oLLM}, such as \textit{Phi4} and \textit{Gemma2}, achieved competitive performance in this experiment without relying on advanced techniques like \ac{CoT}, cherry-picked examples for few-shot prompting, or defaulting to larger parameter count variants.

%\subsubsection{Interference speeds}
%In conclusion, the inference speed reflects the average time each model takes to process a prompt. While some models may be slower but provide better results, others might be faster. However, the reported speeds are based on shared resources, affecting reliability. Despite using a dedicated GPU, shared system components like the CPU and RAM influenced the performance. Overall, all models are fast, but Phi3.5 and Phi4 were slower.