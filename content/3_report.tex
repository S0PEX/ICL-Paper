This section outlines the task setup, including the task definition, dataset, and models used in the experiments. It begins with an introduction to the task and dataset, followed by a detailed description of the models employed. The section then emphasizes the conducted experiments, concluding with an analysis of the obtained results.

\subsection{Overview: Task, Dataset, and Models}

\paragraph{Task Definition.}
In this experiment, the task setup follows the \textit{SemEval 2024 Task 9: Brainteaser}~\cite{jiangBRAINTEASERLateralThinking2023} challenge, simulating a participant's approach, but with the limitation of using \acp{oLLM}, in contrast to the original competition, where the majority of participants (90\%) relied on closed-source models, e.g., \acs{GPT}-4~\cite{openaiGPT4TechnicalReport2024}, \acs{GPT}-3.5, and Gemini Pro~\cite{teamGeminiFamilyHighly2024}. As in the official task, evaluation is conducted in a direct question-answering format, where each model is given a puzzle question and is expected to generate the correct answer from a provided list of possible answers. To improve model performance, prompt optimization techniques such as \ac{ICL}~\cite{brownLanguageModelsAre2020} are applied, along with strategies like instructing the model to \textit{"think outside the box"} or \textit{"think step by step"}. No fine-tuning is performed, with the focus instead on leveraging prompting techniques.

\paragraph{Dataset.}
For the dataset, the \citewithtitle{jiangBRAINTEASERLateralThinking2023} dataset by \citeauthor{jiangBRAINTEASERLateralThinking2023} is utilized. The dataset, publicly available on \hreffootnote{https://github.com/1171-jpg/BrainTeaser}{GitHub}, consists of 1,100 lateral thinking puzzles formatted as question-answer pairs. The puzzles are categorized into \textit{Sentence puzzles}, where the puzzle defying commonsense is centered on sentence snippets, and \textit{Word puzzles}, where the answer violates the default meaning of the word and focuses on the letter composition of the target question~\cite{jiangBRAINTEASERLateralThinking2023}. Furthermore, the puzzles are grouped into sets that include original questions as well as their semantic and contextual variations. This grouping allows for a systematic assessment of the models' ability to maintain consistent reasoning across different puzzle formulations~\cite{jiangBRAINTEASERLateralThinking2023}.

Note that while the \textit{SemEval 2024 Task 9} provides an alternative dataset with a predefined training-evaluation split, this experiment follows the original study's approach~\cite{jiangBRAINTEASERLateralThinking2023}, evaluating \acp{LLM} on the full dataset.

\paragraph{Models.}
For model selection, only open-source or at least open-weight models were considered, ensuring that their source code and/or model weights are publicly accessible. Based on these criteria, the following models: \ac{LLaMA} 3.1 and 3.2~\cite{grattafioriLlama3Herd2024}, \acs{Phi} 3.5 and 4~\cite{abdinPhi3TechnicalReport2024, abdinPhi4TechnicalReport2024}, \acs{Qwen}~\cite{qwenQwen25TechnicalReport2025}, \acs{Gemma}~\cite{teamGemma2Improving2024}, and \acs{Mistral}~\cite{MistralNeMoMistral} were chosen. All of these represent the current state of the art, demonstrating strong capabilities in reasoning and problem-solving tasks~\cite{grattafioriLlama3Herd2024, abdinPhi3TechnicalReport2024, abdinPhi4TechnicalReport2024, qwenQwen25TechnicalReport2025, teamGemma2Improving2024, MistralNeMoMistral}.

Each model was evaluated across all available parameter sizes that fit within the 24 GB \ac{VRAM} of an NVIDIA GeForce RTX 3090. For smaller models (fewer than 3B parameters), FP16 precision was used, as they are generally more sensitive to quantization, which involves representing model weights, activations, and the KV cache with lower-precision numerical formats~\cite{liEvaluatingQuantizedLarge2024}. For larger models (up to 14B parameters), Q8\_0 precision was applied where possible, as recent studies suggest no significant performance difference between FP16 and Q8\_0~\cite{raubaQuantifyingPerturbationImpacts2024, liEvaluatingQuantizedLarge2024}. For models exceeding 27B parameters, Q4\_K\_M precision was employed to fit them within \ac{VRAM}. However, using such a heavy quantization method may lead to performance degradation.

\paragraph{Evaluation Metrics.}
Following the official settings by \textcite{jiangBRAINTEASERLateralThinking2023}, model performance is assessed using three accuracy metrics: \textit{1) Instance-based Accuracy}, which measures the model's performance on each individual puzzle. \textit{2) Group-based Accuracy}, which evaluates the model's consistency across the original puzzle and its variations. For Group-based Accuracy, the model receives a score of 1 only if it correctly solves all three variations of a given puzzle, otherwise, the score is 0. Lastly, \textit{3) Overall Accuracy}, which reflects the accuracy across all instances~\cite{jiangBRAINTEASERLateralThinking2023}.

Each metric is reported for both raw and processed model responses. A \textit{raw response} refers to the model's direct output, while a \textit{processed response} involves post-processing to ensure consistent formatting, particularly when the model's answer deviates from the expected format. For example, if the model outputs a full phrase (e.g., \textit{"The best answer for this puzzle is **(A)** The man is a barber"}), the processing step will extract the answer choice "A" and return it as the final response.

\subsection{Experiments}

In all experiments, an enhanced version of the prompt used by \textcite{jiangBRAINTEASERLateralThinking2023} was employed. Specifically, the word \textit{brain teaser} was replaced with \textit{question}. Additionally, the option \textit{"none of the above"} was enclosed in quotes to ensure the model recognizes it as a selectable choice rather than an alternative option.

% \begin{lstlisting}[xleftmargin=0pt, breaklines=true]
% Please select the best answer for the question. Each question has only one correct answer, including the choice 'None of above'. Your answer should only include the choice:

% Question: {question}
% Choice:
% (A) {choice_A}
% (B) {choice_B}
% (C) {choice_C}
% (D) None of the above
% Answer:
% \end{lstlisting}

\paragraph{Zero-Shot Baseline.}
In this experiment, each \ac{LLM} was provided with the default system prompt: \textit{"You are a helpful AI assistant"}. The system prompt serves as an introductory message to guide the behavior of the model. It sets the context for how the model should approach the task and what type of responses are expected. No additional instructions or examples were provided to the model, making this a true zero-shot setting where the model had to rely solely on its pre-trained knowledge to generate answers.

\paragraph{Zero-Shot with Prompt Engineering.}
In this experiment, the same system prompt as the Zero-Shot Baseline was used, with the addition of an instruction for the models to \textit{"\{...\} think step by step \{...\}"} before responding. This technique, a form of prompt engineering, encourages models to break down their reasoning process, potentially leading to more detailed and accurate answers~\cite{weiChainofThoughtPromptingElicits2023}. Other variations of this instruction were also explored, such as prompting the models with \textit{"You are \{...\}. Think step by step to consider hidden meanings and metaphorical interpretations \{...\}"} or \textit{"\{...\} approach each brain teaser from multiple perspectives \{...\}."} These variations were designed to foster creative and multi-faceted reasoning.

\paragraph{Few-Shot.}
For the Few-Shot experiment, the dataset was divided into an evaluation set and a selection pool of examples. Instead of injecting the examples directly into the prompt, the model's API was accessed directly. Each example was provided as user input, followed by the model's answer, which included the correct choice letter. This exchange was then incorporated into the chat history, effectively providing the model with context and helping it learn from the provided examples. This method was chosen because it follows the suggested format by LangChain for effective few-shot learning. The selection of examples was randomized to ensure diversity in answers and prevent the model from learning biases towards a particular answer choice (e.g., always selecting answer "A"). This approach allowed the model to generalize better from a limited number of examples.

\paragraph{Few-Shot with Prompt Engineering.}
In this experiment, the Few-Shot approach was combined with the same prompt engineering techniques as the Zero-Shot with Prompt Engineering experiment. The system prompt was aligned with those used previously, encouraging the models to "think step by step" and consider various formulations. As in the few-shot experiment, each example was injected into the model's chat history as user input followed by the correct answer choice, ensuring that the model had access to a small set of labeled examples while also benefiting from the structured reasoning approach. This setup aimed to combine the benefits of both few-shot learning and prompt engineering to improve the modelâ€™s accuracy and consistency.

\subsection{Evaluation and Results}
