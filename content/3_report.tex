This section outlines the experimental setup, covering the task definition, dataset, and models used. It begins with an introduction to the task and dataset, followed by a detailed description of the models employed. The section then discusses the conducted experiments and concludes with an analysis of the obtained results.

\subsection{Overview: Task, Dataset, and Models}

\paragraph{Task Definition.}
The task setup follows the \textit{SemEval 2024 Task 9: Brainteaser}~\cite{jiangBRAINTEASERLateralThinking2023} challenge, simulating a participant's approach but with the constraint of using \acp{oLLM}. This contrasts with the original competition, where most participants (90\%) relied on closed-source models, such as \acs{GPT}-4~\cite{openaiGPT4TechnicalReport2024}, \acs{GPT}-3.5, and Gemini Pro~\cite{teamGeminiFamilyHighly2024}. As in the official task, evaluation follows a direct question-answering format: each model receives a puzzle question and must generate the correct answer from a provided list. To improve model performance, prompt optimization techniques such as \ac{ICL}~\cite{brownLanguageModelsAre2020} are applied, including additional strategies like instructing the model to \textit{"think outside the box"} or \textit{"think step by step"}. No fine-tuning is performed, instead, the focus is on leveraging prompting techniques.

\paragraph{Dataset.}
The dataset used in this study is the \citewithtitle{jiangBRAINTEASERLateralThinking2023} dataset, publicly available on \hreffootnote{https://github.com/1171-jpg/BrainTeaser}{GitHub}. It consists of 1,100 lateral thinking puzzles formatted as question-answer pairs. The puzzles are categorized into \textit{Sentence Puzzles}, which challenge commonsense expectations based on sentence structures, and \textit{Word Puzzles}, where the answer defies the default meaning of a word and instead focuses on its letter composition~\cite{jiangBRAINTEASERLateralThinking2023}. Additionally, the puzzles are grouped into sets that include the original questions and their semantic and contextual variations, allowing for a systematic assessment of model consistency across different formulations~\cite{jiangBRAINTEASERLateralThinking2023}.

While \textit{SemEval 2024 Task 9} provides an alternative dataset with a predefined training-evaluation split, this experiment follows the approach of the original study~\cite{jiangBRAINTEASERLateralThinking2023}, evaluating \acp{LLM} on the full dataset.

\paragraph{Models.}
Only open-source or open-weight models were considered, ensuring public access to their source code and/or model weights. Based on these criteria, the following models were selected: \ac{LLaMA} 3.1 and 3.2~\cite{grattafioriLlama3Herd2024}, \acs{Phi} 3.5 and 4~\cite{abdinPhi3TechnicalReport2024, abdinPhi4TechnicalReport2024}, \acs{Qwen}~\cite{qwenQwen25TechnicalReport2025}, \acs{Gemma}~\cite{teamGemma2Improving2024}, and \acs{Mistral}~\cite{MistralNeMoMistral}. These models represent the state of the art in reasoning and problem-solving tasks~\cite{grattafioriLlama3Herd2024, abdinPhi3TechnicalReport2024, abdinPhi4TechnicalReport2024, qwenQwen25TechnicalReport2025, teamGemma2Improving2024, MistralNeMoMistral}.

Each model was evaluated across all available parameter sizes that fit within the 24 GB \ac{VRAM} of an NVIDIA GeForce RTX 3090. For smaller models (fewer than 3B parameters), FP16 precision was used, as these models are more sensitive to quantization, which involves representing model weights, activations, and the KV cache in lower-precision formats~\cite{liEvaluatingQuantizedLarge2024}. For larger models (up to 14B parameters), Q8\_0 precision was applied where possible, as recent studies suggest no significant performance difference between FP16 and Q8\_0~\cite{raubaQuantifyingPerturbationImpacts2024, liEvaluatingQuantizedLarge2024}. Models exceeding 27B parameters were evaluated using Q4\_K\_M precision to fit within \ac{VRAM}, though this level of quantization may lead to performance degradation.

\paragraph{Evaluation Metrics.}
Following the official settings by \textcite{jiangBRAINTEASERLateralThinking2023}, model performance was assessed using three accuracy metrics: \textit{(1) Instance-based Accuracy}, which measures the model's performance on each individual puzzle; \textit{(2) Group-based Accuracy}, which evaluates consistency across the original puzzle and its variations (where a model scores 1 only if it correctly solves all three variations of a puzzle, otherwise scoring 0); and \textit{(3) Overall Accuracy}, which reflects accuracy across all instances~\cite{jiangBRAINTEASERLateralThinking2023}.

Each metric is reported for both raw and processed model responses. A \textit{raw response} refers to the model's direct output, whereas a \textit{processed response} involves post-processing to ensure consistent formatting, particularly when the model's answer deviates from the expected format. For example, if the model outputs a full phrase (e.g., \textit{"The best answer for this puzzle is **(A)** The man is a barber"}), the processing step extracts the answer choice "A" as the final response.

\subsection{Experiments}

In all experiments, an enhanced version of the prompt used by \textcite{jiangBRAINTEASERLateralThinking2023} was employed. Specifically, the word \textit{brain teaser} was replaced with \textit{question} to align with standard terminology. Additionally, the option \textit{"none of the above"} was enclosed in quotes to ensure the model recognizes it as a selectable choice rather than an alternative option.

\paragraph{Zero-Shot Baseline.}
\label{zero-shot-prompt}
Each \ac{LLM} was evaluated in a zero-shot setting using the default system prompt: \textit{"You are a helpful AI assistant."} This system prompt provides a general instruction for the model's behavior without introducing task-specific details. No additional instructions or examples were included, requiring the model to rely solely on its pre-trained knowledge when generating answers.

\paragraph{Zero-Shot with Prompt Engineering.}
\label{zero-shot-prompt-engineering}
Building upon the \nameref{zero-shot-prompt} experiment, this experiment introduced prompt engineering techniques designed to enhance reasoning. The system prompt was modified to include additional instructions encouraging lateral thinking. For example, the system prompt was extended with phrases such as \textit{"You are \{...\}. Think step by step \{...\}"} to promote lateral reasoning. Alternative variations included instructions like \textit{"You are \{...\}. Think step by step to consider hidden meanings and metaphorical interpretations \{...\}"} or \textit{"\{...\} approach each question from multiple perspectives \{...\}"}. These refinements aimed to improve interpretability and accuracy by guiding the model's cognitive process. Each system prompt variation was initially tested on a small 10\% random sample set, as running all variations on the complete dataset was to costly and time-consuming. The results of the small experiment were then used to obtain the best system prompt ($s_i$) for each model ($m_i$). Then, each model $m_i$ was prompted with $s_i$ on the complete dataset.

\paragraph{Few-Shot.}
\label{few-shot-prompt}
In this experiment, a subset of the dataset was allocated for $n$-few-shot prompting. In few-shot prompting, a small number of examples are provided within the prompt to guide the model's responses~\cite{brownLanguageModelsAre2020}. Adopting a conversational approach suitable for chat-based models, examples were presented as a chat history to the model. Each example consisted of a user query paired with the corresponding model answer. This methodology aligns with best practices for few-shot prompting in chat models, as outlined by \hreffootnote{https://python.langchain.com/docs/how\_to/few\_shot\_examples\_chat/}{LangChain}~\cite{HowUseFew}. To mitigate biases, such as the model favoring a particular answer choice like "A" due to its overall frequency in the prepended examples, examples were randomly selected while ensuring diversity in the answer label as long as possible. The best number of examples ($n_i$) to provide to a model ($m_i$) was initially tested on a small 10\% random sample set, as running all $n \in [1, 10]$ on the complete dataset was to costly and time-consuming. The results of the small experiment were then used to obtain a unique $n_i$ for each model. Then, each model $m_i$ was prompted with $n_i$ examples on the complete dataset.

\paragraph{Few-Shot with Prompt Engineering.}
This experiment combined few-shot learning with the prompt engineering techniques introduced in \nameref{zero-shot-prompt-engineering}. The system prompt was extended with instructions encouraging step-by-step analysis. As in the previous \nameref{few-shot-prompt} experiment, examples were presented in a conversational format, integrating both user queries and correct responses before the user query. Each model ($m_i$) was prompted with the best number of examples ($n_i$) and the best system prompt ($s_i$) obtained from the zero-shot experiments.

\subsection{Evaluation and Results}
