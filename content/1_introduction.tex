In recent years, the development of \acp{LLM} has significantly advanced the field of \ac{NLP}. These models are capable of processing and generating human-like text, effectively handling tasks that require logical reasoning. However, lateral thinking, essential for creativity and solving unconventional problems, remains a challenge. To evaluate how well language models address such challenges, the SemEval 2024 BRAINTEASER task was introduced.

This paper analyzes various LLMs for the BRAINTEASER task, focusing on models such as LLaMA, Phi, and Qwen. The objective is to compare their performance and understand how different versions and sizes influence their ability to tackle lateral thinking tasks. An additional focus is on \ac{ICL}, which enables models to enhance their responses using only prompts without requiring additional training. By exploring different prompting techniques, this study aims to identify optimal strategies for improving LLMs in complex reasoning scenarios.

Recent iterations of models, such as LLaMA 3.1 and 3.2, have shown notable improvements in creative problem-solving abilities. These advancements make them particularly interesting for evaluating lateral thinking tasks within the BRAINTEASER framework.

This paper aims to provide insights into the strengths and limitations of current LLMs in performing lateral thinking tasks, contributing to the development of more advanced models capable of human-like creative reasoning.