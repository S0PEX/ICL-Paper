In recent years, \acp{LLM} have made significant strides, demonstrating impressive capabilities in natural language processing and a variety of complex tasks~\cite{naveedComprehensiveOverviewLarge2024}. Pre-trained models, such as GPT-3, have showcased their ability to perform complex tasks directly from text prompts, without the need for task-specific fine-tuning~\cite{brownLanguageModelsAre2020}. This progress is especially noticeable in areas that require logical reasoning and structured problem-solving, such as solving mathematical problems and logical puzzles~\cite{panLogicLMEmpoweringLarge2023,zebazeTreeProblemsImproving2024,yuFlowReasoningTraining2025}. However, despite their proficiency in logical reasoning, it remains uncertain whether these models can engage in creative reasoning processes similar to humans~\cite{jiangBRAINTEASERLateralThinking2023}.

Human reasoning can be divided into two primary categories: vertical and lateral thinking. Vertical thinking, also known as logical or linear thinking, involves a structured, step-by-step approach based on rationality, logic, and well-established rules~\cite{jiangBRAINTEASERLateralThinking2023}. In contrast, lateral thinking, often referred to as "thinking outside the box," is a more creative and divergent process that challenges traditional approaches and explores alternative perspectives~\cite{jiangBRAINTEASERLateralThinking2023}.

Although \acp{LLM} have excelled at tasks involving complex logical reasoning, most research has focused on vertical thinking, leaving lateral thinking tasks largely unexplored~\cite{jiangBRAINTEASERLateralThinking2023,chenWeakevalStrongEvaluatingEliciting2024,huangLatEvalInteractiveLLMs2024}. To address this gap, \textcite{jiangBRAINTEASERLateralThinking2023} introduced the BrainTeaser task, a multiple-choice question-answering challenge designed to evaluate the ability of models to engage in lateral thinking. Based on the BrainTeaser benchmark~\cite{jiangBRAINTEASERLateralThinking2023}, this task requires models to solve problems by thinking creatively and from unconventional angles. Unlike other benchmarks like RiddleSense~\cite{linRiddleSenseReasoningRiddle2021}, which focus on commonsense reasoning, BrainTeaser challenges models to break free from default assumptions and linear reasoning paths, thus testing their capacity for creative problem-solving~\cite{jiangBRAINTEASERLateralThinking2023}.

\textcite{jiangBRAINTEASERLateralThinking2023} evaluated models such as Flan-T5, RoBERTa, and \acs{GPT}~3.5, but did not include open models like \acs{LLaMA}, \acs{Phi}, and \acs{Qwen}~\cite{jiangBRAINTEASERLateralThinking2023}. These open models have demonstrated strong performance on various benchmarks, including \ac{MATH}~\cite{OpenLLMLeaderboard}, raising the question of how well they perform on the BrainTeaser task.

This paper investigates the performance of six open \acp{LLM} on the BrainTeaser task, comparing their results to those of the original models tested by \textcite{jiangBRAINTEASERLateralThinking2023}, including \acs{GPT}~3.5. The analysis focuses on both their initial performance and improvements achieved through prompt engineering, particularly using \ac{ICL} methods like Few-Shot Learning. These techniques guide the model by providing examples within the prompt, enhancing performance without additional fine-tuning~\cite{yinDeeperInsightsUpdates2024,brownLanguageModelsAre2020}. By analyzing these models on the BrainTeaser task, this paper offers important insights into their strengths and limitations, contributing to a deeper understanding of their creative reasoning abilities.
