In recent years, \acp{LLM} have made significant progress in solving a variety of reasoning and problem-solving tasks without the need for fine-tuning~\cite{radfordLanguageModelsAre2019}. These advancements enable the models to perform well in areas such as mathematical reasoning, logical inference, and commonsense understanding~\cite{panLogicLMEmpoweringLarge2023, qiaoReasoningLanguageModel2023, naveedComprehensiveOverviewLarge2024}. Techniques like \ac{ICL}~\cite{brownLanguageModelsAre2020}, i.e., learning from a few examples in the model's prompt, and \ac{CoT} prompting~\cite{weiChainofThoughtPromptingElicits2023}, which enhances reasoning through step-by-step explanations, have further boosted their ability to tackle complex tasks by enabling greater generalization through prompt optimization~\cite{brownLanguageModelsAre2020,weiChainofThoughtPromptingElicits2023}. However, most research on these advancements primarily focuses on vertical thinking, a problem-solving approach that follows a structured logical step-by-step process, where each step logically builds upon the previous one~\cite{jiangBRAINTEASERLateralThinking2023}.

In contrast, lateral thinking requires a creative and unconventional approach to problem-solving. Unlike vertical thinking, lateral thinking involves breaking away from typical reasoning patterns and exploring alternative interpretations~\cite{jiangBRAINTEASERLateralThinking2023, jiangSemEval2024Task92024}. While many datasets exist for commonsense reasoning and structured problem-solving, lateral thinking tasks have received significantly less attention~\cite{jiangBRAINTEASERLateralThinking2023, jiangSemEval2024Task92024}. Most AI research has focused on improving logical reasoning, while lateral thinking problems are often overlooked and treated as irrelevant~\cite{jiangBRAINTEASERLateralThinking2023, jiangSemEval2024Task92024}. As a result, the ability of \acp{LLM} to handle lateral thinking tasks is still not well-explored.

To address this gap, \textcite{jiangBRAINTEASERLateralThinking2023} introduced \citetitle{jiangBRAINTEASERLateralThinking2023}, a benchmark designed to evaluate lateral thinking abilities. This benchmark features problems that challenge common sense and require unconventional reasoning, diverging from typical reasoning patterns. At SemEval 2024 Task 9~\cite{jiangSemEval2024Task92024}, \citetitle{jiangBRAINTEASERLateralThinking2023} was expanded into a competition format, where participants explored various methods, including fine-tuning, \ac{ICL}, and other prompting techniques, such as \ac{CoT}, to enhance model performance in lateral reasoning tasks.

An important finding from the competition was that most participants (90\%) relied on closed-source \acp{LLM}~\cite{jiangSemEval2024Task92024}, such as OpenAI's \acs{GPT}-4~\cite{openaiGPT4TechnicalReport2024}, \acs{GPT}-3.5, and Gemini Pro~\cite{teamGeminiFamilyHighly2024}. In contrast, newer \acp{oLLM}, such as \ac{LLaMA}~\cite{grattafioriLlama3Herd2024}, \ac{Gemma}~\cite{teamGemma2Improving2024}, and \ac{Qwen}~\cite{qwenQwen25TechnicalReport2025}, were not evaluated. Given the recent advancements in reasoning demonstrated by such open models, further exploration of their capabilities in lateral thinking is warranted~\cite{OpenLLMLeaderboard, grattafioriLlama3Herd2024, teamGemma2Improving2024, qwenQwen25TechnicalReport2025}.

In this paper, I investigate the performance of \acp{oLLM}, such as \ac{LLaMA}, \ac{Qwen}, \ac{Phi}, and others, in comparison to previously used closed models on \textit{SemEval 2024 Task 9: BRAINTEASER}. Additionally, I examine the role of \ac{ICL} in enhancing these models lateral thinking abilities through prompt optimization. Through this analysis, I aim to provide insights into the potential of \acp{oLLM} for tackling lateral thinking tasks.
