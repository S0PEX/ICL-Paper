In recent years, \acp{LLM} have made significant progress, demonstrating strong capabilities in natural language processing and other complex tasks~\cite{naveedComprehensiveOverviewLarge2024}. Pre-trained models like GPT-3 have shown the ability to perform intricate tasks directly from text prompts, without requiring task-specific fine-tuning~\cite{brownLanguageModelsAre2020}. This progress is particularly evident in areas that demand logical reasoning and structured problem-solving, such as solving math problems and logical puzzles~\cite{panLogicLMEmpoweringLarge2023,zebazeTreeProblemsImproving2024,yuFlowReasoningTraining2025}. However, despite their proficiency in logical reasoning, it remains unclear whether these models can engage in creative reasoning in a manner similar to humans~\cite{jiangBRAINTEASERLateralThinking2023}.

Human reasoning can be categorized into two main types: vertical and lateral thinking. Vertical thinking, also known as linear or logical thinking, involves a sequential and analytical approach based on rationality, logic, and established rules~\cite{jiangBRAINTEASERLateralThinking2023}. On the other hand, lateral thinking, often referred to as ``thinking outside the box``, is a creative and divergent process that challenges conventional methods and explores alternative perspectives~\cite{jiangBRAINTEASERLateralThinking2023}.

While \acp{LLM} have achieved success in solving complex logical problems, most studies have focused on vertical thinking. As a result, there has been little research on how well these models handle lateral thinking tasks~\cite{jiangBRAINTEASERLateralThinking2023,chenWeakevalStrongEvaluatingEliciting2024,huangLatEvalInteractiveLLMs2024}. To explore this, \textcite{jiangBRAINTEASERLateralThinking2023} introduced \citetitle{jiangBRAINTEASERLateralThinking2023}, a set of multiple-choice questions designed to test if a model can perform lateral thinking and avoid common assumptions.

\textcite{jiangBRAINTEASERLateralThinking2023} evaluated models such as Flan-T5, RoBERTa, and \acs{GPT}~3.5, but did not include open models like \acs{LLaMA}, \acs{Phi}, and \acs{Qwen}~\cite{jiangBRAINTEASERLateralThinking2023}. These open models have demonstrated superior performance on various benchmarks, including \ac{MATH}, raising the question of how well they perform on the \citetitle{jiangBRAINTEASERLateralThinking2023} task~\cite{OpenLLMLeaderboard}.

This paper investigates the performance of six open \acp{LLM} on the \citetitle{jiangBRAINTEASERLateralThinking2023} task, comparing their results with those of the original models tested by \textcite{jiangBRAINTEASERLateralThinking2023}, including \acs{GPT}~3.5. The analysis focuses on both their initial performance and improvements achieved through prompt engineering, particularly using \ac{ICL} methods like Few-Shot Learning. These techniques guide the model by providing examples within the prompt, enhancing accuracy without additional training~\cite{yinDeeperInsightsUpdates2024,brownLanguageModelsAre2020}. By analyzing these models on the \citetitle{jiangBRAINTEASERLateralThinking2023} task, this paper offers valuable insights into their strengths and limitations, contributing to the understanding of their capabilities in creative reasoning tasks.
