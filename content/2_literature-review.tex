\textbf{Lateral Thinking in \acp{LLM}}

Historically, the evaluation of \acp{LLM} focused on vertical thinking, which involves logical, step-by-step reasoning often seen in tasks like math problems. These evaluations aimed to optimize the models' ability to perform sequential, deterministic tasks. However, recent advancements have led to the development of new frameworks and benchmarks that place an increasing emphasis on lateral thinking, which challenges \acp{LLM} to think creatively and outside the box.

One of the early benchmarks to introduce lateral thinking was \citewithtitle{jiangBRAINTEASERLateralThinking2023}, which featured brainteasers in a question-and-answer style. These tasks, similar in nature to the riddles in \citewithtitle{linRiddleSenseReasoningRiddle2021}, assess a model's ability to engage in commonsense reasoning and creative problem-solving. While both focus on non-trivial problem-solving, \citewithtitle{jiangBRAINTEASERLateralThinking2023} more directly targets lateral thinking.

Building on these foundations, more advanced frameworks such as \citewithtitle{huangLatEvalInteractiveLLMs2024} and \citewithtitle{chenWeakevalStrongEvaluatingEliciting2024} introduced interactive environments for testing lateral thinking. These models go beyond static question-answer pairs, creating dynamic problem-solving contexts that more thoroughly evaluate the creativity and flexibility of \acp{LLM} in complex tasks.

\textbf{Performance in Lateral Thinking Benchmarks}

The performance of \acp{LLM} in lateral thinking benchmarks has shown a significant gap compared to human performance. For instance, in the \citetitle{jiangBRAINTEASERLateralThinking2023} benchmark, the best-performing model, \ac{GPT}-3, achieved only 53\% accuracy on word puzzles and 63\% on sentence puzzles. In contrast, human participants scored approximately 92\% accuracy on the same tasks. This indicates that while \acp{LLM} can perform reasonably well, they still fall short of human-level lateral thinking. Furthermore, models fine-tuned on commonsense knowledge, such as RoBERTa-L fine-tuned on CSKG, performed worse than their base counterparts, suggesting that heavy reliance on commonsense reasoning may actually hinder lateral thinking capabilities in some scenarios.

Similarly, the \citetitle{huangLatEvalInteractiveLLMs2024} benchmark demonstrated that even \ac{GPT}-4, the most advanced model at the time, struggled with interactive lateral thinking tasks. In this framework, models engage in iterative problem-solving by asking yes-or-no questions to refine their understanding of incomplete scenarios. Despite being able to ask relevant questions, models often struggled to synthesize the information into coherent conclusions. This highlights the challenge of interactive problem-solving, where reasoning must evolve through multiple steps of question and answer.

In comparison, \citetitle{linRiddleSenseReasoningRiddle2021} primarily tests the ability of models to solve riddles using commonsense reasoning. This task presents an easier challenge for \acp{LLM}, with models typically outperforming human participants on standard riddles, as these tasks often require more straightforward reasoning rather than creative, lateral thought.

\textbf{Advanced Lateral Thinking Frameworks}

In more advanced frameworks like \citetitle{chenWeakevalStrongEvaluatingEliciting2024}, the focus shifts from basic puzzle solving to evaluating how well models engage with complex, situation-based puzzles. The framework includes 975 graded puzzles spanning three difficulty levels—easy, medium, and hard. These puzzles require increasingly creative reasoning as their difficulty progresses, with models engaging in a player-judge system where the model (player) interacts with an evaluation model (judge) through a series of questions and answers. This approach not only evaluates existing lateral thinking abilities but actively elicits and refines these skills through interactive engagement.

\textbf{Challenges and Insights}

Despite the progress in developing lateral thinking benchmarks, the performance of \acp{LLM} remains below human capabilities. While simpler setups, such as \citetitle{jiangBRAINTEASERLateralThinking2023} and \citetitle{linRiddleSenseReasoningRiddle2021}, yield relatively better performance from models, more advanced frameworks like \citetitle{huangLatEvalInteractiveLLMs2024} and \citetitle{chenWeakevalStrongEvaluatingEliciting2024} expose the limitations of \acp{LLM} in creative problem-solving. The need to interpret incomplete or ambiguous information, ask relevant questions, and maintain consistency across puzzle variants remains a significant challenge. These frameworks demand a higher degree of creativity and reasoning, making it clear that while \acp{LLM} excel in vertical thinking tasks, they still struggle with lateral thinking, which involves more complex, creative reasoning.

\textbf{Conclusion}

Recent benchmarks highlight both the progress and limitations in evaluating the lateral thinking abilities of \acp{LLM}. While simpler, direct question-answer tasks—such as those found in \textit{BRAINTEASER}~\cite{jiangBRAINTEASERLateralThinking2023} and \textit{RiddleSense}~\cite{linRiddleSenseReasoningRiddle2021}—allow \acp{LLM} to perform better, more advanced systems, such as \textit{LatEval}~\cite{huangLatEvalInteractiveLLMs2024} and \textit{Weak-eval-Strong}~\cite{chenWeakevalStrongEvaluatingEliciting2024}, expose the models’ limitations in creative problem-solving. These results underscore the need for continued research to develop frameworks that better challenge \acp{LLM} in lateral thinking and foster their ability to solve complex, non-linear problems.
