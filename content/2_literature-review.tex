Lateral thinking evaluation for \acp{LM} has gained increasing attention in recent years~\cite{jiangBRAINTEASERLateralThinking2023, huangLatEvalInteractiveLLMs2024, chenWeakevalStrongEvaluatingEliciting2024}. One of the first benchmarks in this area was \citetitler{jiangBRAINTEASERLateralThinking2023}, which introduced a structured approach to testing lateral reasoning in \acp{LLM}. Subsequent research, e.g., \citetitler{huangLatEvalInteractiveLLMs2024} and \citetitler{chenWeakevalStrongEvaluatingEliciting2024}, has built upon this foundation, adopting more advanced setups that integrate additional reasoning mechanisms and interactive problem-solving elements.

\subsection{BRAINTEASER: A Benchmark for Lateral Thinking in Language Models}

\citetitle{jiangBRAINTEASERLateralThinking2023} by \textcite{jiangBRAINTEASERLateralThinking2023} is a multiple-choice \ac{QA} benchmark designed to evaluate the lateral thinking capabilities of \acp{LM}. Unlike conventional vertical thinking tasks that emphasize logical inference and commonsense reasoning, \citetitle{jiangBRAINTEASERLateralThinking2023} challenges models to override default commonsense associations and engage in creative, non-linear problem-solving~\cite{jiangBRAINTEASERLateralThinking2023}. The experimental setup involves evaluating \acp{LM} on sentence- and word-level lateral thinking puzzles to assess their ability to think \textit{``outside the box''}. This approach is similar to \citetitler{linRiddleSenseReasoningRiddle2021}, which also examines linguistic creativity via riddles but primarily tests complex commonsense reasoning rather than lateral thinking.

\textbf{Research Design}
The dataset comprises 1,100 lateral thinking puzzles, structured as question-answer pairs. These puzzles are categorized into two distinct types: \textit{Sentence Puzzles}, which require narrative-based reasoning, and \textit{Word Puzzles}, which involve lexical manipulation tasks~\cite{jiangBRAINTEASERLateralThinking2023}. Furthermore, the puzzles are organized into sets that include original questions as well as their semantic and contextual variations. This structure allows for a systematic assessment of the models' ability to maintain consistent reasoning across different puzzle formulations. The dataset is provided to the models in a direct question-answer format, where the prompt consists of the puzzle question and the expected output is the model's answer~\cite{jiangBRAINTEASERLateralThinking2023}.

\textbf{Evaluation Metrics}
\citeauthor{jiangBRAINTEASERLateralThinking2023} utilizes two accuracy-based metrics:
\textit{Instance-based Accuracy}, which evaluates each riddle question individually, and \textit{Group-based Accuracy}, which assesses the model's consistency across all variations (original, semantic, and contextual reconstructions) of a given puzzle. In this metric, the model scores 1 if it correctly solves all variations of a given puzzle.

\textbf{Findings}
Evaluating the results revealed a substantial performance gap between \acp{LM} and human participants. While humans achieve an overall accuracy of approximately 92\%, the best-performing \ac{LLM}, ChatGPT, only achieves 53\% on word puzzles and 63\% on sentence puzzles, thus positioning it midway between human performance and random chance~\cite{jiangBRAINTEASERLateralThinking2023}. Notably, models struggle to maintain consistency across puzzle variants, indicating weak generalization of lateral thinking. Additionally, models fine-tuned on commonsense knowledge (e.g., RoBERTa-L on CSKG) perform worse than their vanilla counterparts, suggesting that reliance on commonsense associations may hinder lateral reasoning~\cite{jiangBRAINTEASERLateralThinking2023}.

\subsection{LatEval: An Interactive Evaluation Framework}

\citetitle{huangLatEvalInteractiveLLMs2024} by \textcite{huangLatEvalInteractiveLLMs2024} is an evaluation benchmark designed to assess the lateral thinking capabilities of \ac{LLM} within an interactive framework. Unlike traditional benchmarks that emphasize direct question-answering tasks, \citetitle{huangLatEvalInteractiveLLMs2024} challenges models to actively pose questions, integrate incomplete information, and deduce solutions through reasoning~\cite{huangLatEvalInteractiveLLMs2024}. The experimental setup simulates the structure of Lateral Thinking Puzzles, where a model (the "player") engages in iterative questioning with a host (another model, typically a more powerful \ac{LLM} such as GPT-4). The player must refine its understanding of the problem through questioning before attempting a final deduction.

\textbf{Research Design}
The dataset comprises 325 high-quality lateral thinking puzzles, selected from a pool of over 2,000 samples in both English and Chinese. Each puzzle consists of an incomplete narrative (the "puzzle"), its corresponding hidden solution (the "truth"), and a set of key clues essential for reasoning~\cite{huangLatEvalInteractiveLLMs2024}. The dataset is presented to models in an interactive format, where the player must pose yes-or-no questions to extract critical information before making a deduction~\cite{huangLatEvalInteractiveLLMs2024}.

\textbf{Evaluation Metrics}
To evaluate the performance of the models, the following metrics are used: \textit{Answer Consistency (AC)} measures how well the final deduction aligns with the truth, \textit{Question Relevance (QR)} evaluates if the player's questions contribute to uncovering the truth~\cite{huangLatEvalInteractiveLLMs2024}, \textit{Question Divergence (QD)} assesses the diversity of the player's questions, reflecting lateral thinking, and \textit{Average Turns (AT)} tracking the number of questions before the final deduction~\cite{huangLatEvalInteractiveLLMs2024}.

\textbf{Findings}
The experiment revealed that most \acp{LLM} struggle with lateral thinking during interactive reasoning tasks~\cite{huangLatEvalInteractiveLLMs2024}. Even the most advanced model, GPT-4, only achieves moderate success. While most models can ask relevant questions, they often struggle to make coherent deductions. Notably, there is a large gap between the relevance of the questions and the consistency of the answers, suggesting that models face challenges in synthesizing the extracted information into accurate conclusions~\cite{huangLatEvalInteractiveLLMs2024}.

\subsection{Weak-eval-Strong: Evaluating and QEliciting Lateral Thinking}

The \citetitle{chenWeakevalStrongEvaluatingEliciting2024} framework by \textcite{chenWeakevalStrongEvaluatingEliciting2024} utilizes situation puzzles to evaluate and enhance the lateral thinking abilities of \acp{LLM}. This framework presents scenarios that require creative and indirect reasoning to uncover underlying explanations, encompassing 975 graded situation puzzles across three difficulty levels: \textins{easy}: which involves simple scenarios requiring straightforward lateral reasoning; \textit{medium}: includes moderately complex situations necessitating more nuanced thinking; and \textit{hard}: consists of complex scenarios demanding advanced lateral thinking and problem-solving skills~\cite{chenWeakevalStrongEvaluatingEliciting2024}. Additionally, the framework employs a player-judge system, wherein the LLM (player) interacts with an evaluation model (judge) through a series of questions and answers, closely mimicking human problem-solving interactions. This approach not only evaluates existing lateral thinking capabilities but also actively elicits and enhances these skills through interactive engagement~\cite{chenWeakevalStrongEvaluatingEliciting2024}.

\subsection{Conclusion}

In recent years, more and more benchmarks have emerged to evaluate the lateral thinking abilities of language models. Simpler setups, such as \citetitle{linRiddleSenseReasoningRiddle2021} and \citetitle{jiangBRAINTEASERLateralThinking2023}, focus on direct question-answering tasks, where the model is given a puzzle and expected to provide an immediate answer~\cite{linRiddleSenseReasoningRiddle2021, jiangBRAINTEASERLateralThinking2023}. These setups tend to yield better performance from the models. In contrast, more advanced frameworks like \citetitle{chenWeakevalStrongEvaluatingEliciting2024} and \citetitle{huangLatEvalInteractiveLLMs2024} employ a turn-based system, where the model interacts with a host, asking questions to gather information that could guide it toward the correct answer~\cite{huangLatEvalInteractiveLLMs2024, chenWeakevalStrongEvaluatingEliciting2024}. However, the model must creatively interpret and synthesize incomplete or ambiguous information to arrive at a solution.

Although the benchmarks discussed above represent just a small sample of the growing body of research, they highlight an important trend: While \acp{LLM} demonstrate certain lateral thinking abilities, their performance in this area still lags behind their capabilities in vertical thinking~\cite{linRiddleSenseReasoningRiddle2021, jiangBRAINTEASERLateralThinking2023, huangLatEvalInteractiveLLMs2024,chenWeakevalStrongEvaluatingEliciting2024}.
