Assessing the lateral thinking capabilities of \acp{LLM} is essential for enhancing their creative problem-solving skills. This evaluation provides insights into how effectively these models can engage in unconventional reasoning, a hallmark of human-like thought. To facilitate this, various specialized benchmarks and evaluation frameworks have been developed, each employing distinct methods and puzzles to challenge the lateral reasoning abilities of \acp{LLM}.

\subsection{RiddleSense: A Benchmark for Riddle Solving}

\citetitle{linRiddleSenseReasoningRiddle2021}, introduced by \textcite{linRiddleSenseReasoningRiddle2021}, is a benchmark designed to assess the performance of \acp{LLM} in solving riddles that require creative and lateral thinking. The dataset consists of 5,700 multiple-choice questions, each paired with human-annotated explanations outlining the reasoning behind the correct answers.

An example from the dataset is the riddle: \textit{"I have five fingers but I am not alive. What am I?"}, with the correct answer being \textit{"a glove."}.

Solving these riddles demands commonsense reasoning, understanding figurative language, and counterfactual reasoning~\cite{linRiddleSenseReasoningRiddle2021}. Commonsense reasoning allows for drawing logical conclusions from everyday knowledge, figurative language comprehension is essential for interpreting metaphors and idioms, and counterfactual reasoning involves evaluating hypothetical scenarios. These skills are critical for advancing \ac{NLU}~\cite{linRiddleSenseReasoningRiddle2021}.

Despite the complexity of the tasks, the benchmark reveals a significant gap in performance between humans and language models. While human participants achieved an accuracy rate of 91.33\% on this benchmark, the top-performing language models reached only 68.80\% accuracy~\cite{linRiddleSenseReasoningRiddle2021}. This gap highlights the need for further research into commonsense reasoning and linguistic creativity to improve the performance of machine learning models.

\subsection{BRAINTEASER Benchmark}

The \citetitle{jiangBRAINTEASERLateralThinking2023} Benchmark by \textcite{jiangBRAINTEASERLateralThinking2023} evaluates the lateral thinking abilities of \acp{LLM} through puzzles that challenge common-sense reasoning. It features two main puzzle types: Sentence Puzzles; scenarios with outcomes that defy expectations, requiring unconventional reasoning. Word Puzzles; tasks involving creative manipulation of word meanings and structures.

The benchmark employs two accuracy metrics: Standard Accuracy; measures the percentage of puzzles answered correctly. Consistency-Adjusted Accuracy; assesses the consistency of responses across adversarial formats, providing a robust evaluation of lateral thinking capabilities.

These metrics offer a comprehensive assessment of an \ac{LLM}'s ability to handle lateral thinking tasks, highlighting both its problem-solving skills and the reliability of its reasoning processes.

\subsection{LatEval: An Interactive Evaluation Framework}

\textcite{huangLatEvalInteractiveLLMs2024} introduces \citetitle{huangLatEvalInteractiveLLMs2024}, an interactive benchmark designed to assess \acp{LLM}'s lateral thinking abilities through a lateral thinking puzzle game. In this game, the model (player) poses questions to a host model (interlocutor) to deduce answers. This framework challenges models to generate unconventional questions that deepen its understanding of puzzle scenarios and to synthesize incomplete or ambiguous information to derive creative solutions.

The evaluation utilizes four metrics to measure the models' lateral thinking capabilities:

\textit{Answer Consistency (AC)}: Assesses the consistency between the model's answer, i.e., deducted understanding, and the actual truth.
\textit{Question Relevance (QR)}: Evaluates the relevance of the questions posed by the model during the problem-solving process.
\textit{Question Divergence (QD)}: Measures the diversity of questions generated, indicating the model's ability to think outside conventional norms and explore other perspectives.
\textit{Average Turns (AT)}: Measures the average number of turns taken by the model to solve the puzzle.

In their experiments, the authors evaluated several leading \acp{LLM}, including GPT-4, Llama 2, and Claude. The results revealed that while these models could pose relevant questions (high QR scores) and explored different perspectives (high QD scores), they lacked consistency in their answers (low AC scores). Specifically, GPT-4 achieved scores of AC=27.6, QR=72.7, and QD=79.6. These findings highlight significant gaps in lateral thinking abilities compared to human performance, underscoring the need for further research to enhance models' creativity and adaptability in open-ended problem-solving tasks.

\subsection{Weak-eval-Strong: Evaluating and Eliciting Lateral Thinking}

The \citetitle{chenWeakevalStrongEvaluatingEliciting2024} framework by \textcite{chenWeakevalStrongEvaluatingEliciting2024} utilizes situation puzzles to evaluate and enhance the lateral thinking abilities of \acp{LLM}. This framework presents scenarios that require creative and indirect reasoning to uncover underlying explanations, encompassing 975 graded situation puzzles across three difficulty levels: \textins{easy}: which involves simple scenarios requiring straightforward lateral reasoning; \textit{medium}: includes moderately complex situations necessitating more nuanced thinking; and \textit{hard}: consists of complex scenarios demanding advanced lateral thinking and problem-solving skills~\cite{chenWeakevalStrongEvaluatingEliciting2024}. Additionally, the framework employs a player-judge system, wherein the LLM (player) interacts with an evaluation model (judge) through a series of questions and answers, closely mimicking human problem-solving interactions. This approach not only evaluates existing lateral thinking capabilities but also actively elicits and enhances these skills through interactive engagement~\cite{chenWeakevalStrongEvaluatingEliciting2024}.

\subsection{Conclusion}

In recent years, more and more benchmarks have emerged to evaluate the lateral thinking abilities of language models. Simpler setups, such as \citetitle{linRiddleSenseReasoningRiddle2021} and \citetitle{jiangBRAINTEASERLateralThinking2023}, focus on direct question-answering tasks, where the model is given a puzzle and expected to provide an immediate answer~\cite{linRiddleSenseReasoningRiddle2021, jiangBRAINTEASERLateralThinking2023}. These setups tend to yield better performance from the models. In contrast, more advanced frameworks like \citetitle{chenWeakevalStrongEvaluatingEliciting2024} and \citetitle{huangLatEvalInteractiveLLMs2024} employ a turn-based system, where the model interacts with a host, asking questions to gather information that could guide it toward the correct answer~\cite{huangLatEvalInteractiveLLMs2024, chenWeakevalStrongEvaluatingEliciting2024}. However, the model must creatively interpret and synthesize incomplete or ambiguous information to arrive at a solution.

Although the benchmarks discussed above represent just a small sample of the growing body of research, they highlight an important trend: While \acp{LLM} demonstrate certain lateral thinking abilities, their performance in this area still lags behind their capabilities in vertical thinking~\cite{linRiddleSenseReasoningRiddle2021, jiangBRAINTEASERLateralThinking2023, huangLatEvalInteractiveLLMs2024,chenWeakevalStrongEvaluatingEliciting2024}.
