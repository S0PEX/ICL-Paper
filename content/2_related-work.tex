Lateral thinking evaluation for \acp{LM} has attracted significant attention in the recent two years, with several key benchmarks contributing to the development of this field. One of the earliest efforts, the \textit{BRAINTEASER} benchmark by \textcite{jiangBRAINTEASERLateralThinking2023}, introduced a structured approach to assess lateral reasoning in \acp{LM}. This multiple-choice \ac{QA} benchmark evaluates models on lateral thinking puzzles that require non-linear, creative problem-solving, contrasting with traditional tasks that focus on logical inference and commonsense reasoning. The puzzles are categorized into sentence-based and word-based types, and evaluation is conducted using instance- and group-based accuracy metrics, which assess both individual puzzle solutions and consistency across variations~\cite{jiangBRAINTEASERLateralThinking2023}. Findings from this benchmark revealed a performance gap between \acp{LM} and humans, with even the best-performing model, \ac{GPT}, scoring significantly lower than humans, thus positioning it midway between human performance and random chance~\cite{jiangBRAINTEASERLateralThinking2023}.

Building on this, \textcite{huangLatEvalInteractiveLLMs2024} developed the \textit{LatEval} framework, which shifts from traditional question-answering tasks to an interactive evaluation system. In this setup, models (acting as "players") interactively ask yes/no questions to extract necessary information for solving lateral thinking puzzles. The authors found that despite the models' ability to ask relevant questions to get closer to the puzzle's true answer, they struggled with synthesizing the gathered information into coherent conclusions, highlighting challenges in dynamic reasoning processes~\cite{huangLatEvalInteractiveLLMs2024}.

The \textit{Weak-eval-Strong} framework by \textcite{chenWeakevalStrongEvaluatingEliciting2024} also utilizes an interactive setup, i.e., a player-judge evaluation framework, with a graded situation puzzles dataset (SPLAT) to evaluate lateral thinking abilities across varying levels of difficulty. The dataset is divided into three categories: easy, medium, and hard. Interestingly, models performed well only in the easy category, with \ac{GPT}-4 achieving 70.96\% accuracy~\cite{chenWeakevalStrongEvaluatingEliciting2024}. However, as the difficulty increased, the performance dropped drastically, with the accuracy falling below 40\% for the medium difficulty puzzles.

In addition to their own dataset, \textcite{chenWeakevalStrongEvaluatingEliciting2024} evaluated their setup on the \textit{BRAINTEASER} and \textit{RiddleSense} datasets and also tested open models like \ac{LLaMA} 3. Their results showed that the reasoning prompts within their framework led to improvements in the performance of various \acp{LLM}. For instance, \ac{LLaMA} 3 (70B) achieved an overall accuracy of 91.51\% on sentence puzzles and around 79.54\% on word puzzles, demonstrating the effectiveness of their reasoning-driven approach in enhancing lateral thinking capabilities~\cite{chenWeakevalStrongEvaluatingEliciting2024}.

Together, these works illustrate the evolution of lateral thinking evaluation, with simpler setups, such as \textcite{jiangBRAINTEASERLateralThinking2023}, yielding better performance, while more advanced frameworks and datasets like \textcite{huangLatEvalInteractiveLLMs2024} and \textcite{chenWeakevalStrongEvaluatingEliciting2024} expose the limitations of current models in creative, dynamic reasoning tasks. Despite progress in lateral thinking benchmarks, \acp{LM}'s abilities in this area remain significantly behind their performance in more straightforward reasoning tasks.
