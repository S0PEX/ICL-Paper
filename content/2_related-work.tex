Lateral thinking evaluation for \acp{LM} has gained significant attention over the past two years, with several key benchmarks contributing to its development. Before this surge in interest, research primarily focused on vertical thinking benchmarks and datasets, such as \citewithtitle{biskPIQAReasoningPhysical2019} and \citewithtitle{linRiddleSenseReasoningRiddle2021}, which emphasized logical inference and commonsense reasoning. However, lateral thinking, characterized by non-linear and creative problem-solving, has recently been explored in dedicated studies.

One of the earliest contributions in this field is the \textit{BRAINTEASER} benchmark introduced by \textcite{jiangBRAINTEASERLateralThinking2023}. This benchmark provides a structured approach to assessing lateral reasoning in \acp{LM}, employing a multiple-choice \ac{QA} format to evaluate models on lateral thinking puzzles, which differ significantly from traditional logical tasks by using ambiguous language and leaving room for multiple interpretations. The puzzles are categorized into sentence-based and word-based types, and evaluation is conducted using both instance-based and group-based accuracy metrics. These metrics assess individual puzzle solutions as well as consistency across variations~\cite{jiangBRAINTEASERLateralThinking2023}. \citeauthor{jiangBRAINTEASERLateralThinking2023}'s results indicate a considerable performance gap between \acp{LM} and human participants, with even the best-performing model, \ac{GPT}-3.5, scoring significantly lower than humans, placing it between human-level performance and random chance~\cite{jiangBRAINTEASERLateralThinking2023}.

Building upon this foundation, \textcite{huangLatEvalInteractiveLLMs2024} introduced the \textit{LatEval} framework, which departs from traditional question-answering tasks in favor of an interactive evaluation system. In this approach, models function as "players" that ask yes/no questions to gather necessary information before synthesizing their findings into a coherent conclusion. Evaluation results reveal that while models can generate relevant questions, they struggle to integrate the gathered information into well-formed conclusions, underscoring challenges in dynamic reasoning processes~\cite{huangLatEvalInteractiveLLMs2024}.

Similarly, \textcite{chenWeakevalStrongEvaluatingEliciting2024} proposed the \textit{Weak-eval-Strong} framework, which employs a player-judge evaluation setup with a graded situation puzzle dataset (SPLAT~\cite{chenWeakevalStrongEvaluatingEliciting2024}) to assess lateral thinking abilities across varying difficulty levels. The dataset is divided into three categories: easy, medium, and hard. While models performed well in the easy category, achieving an accuracy of 70.96\% with \ac{GPT}-4, performance dropped significantly as difficulty increased, with accuracy falling below 40\% for medium and hard puzzles.

Despite these challenges, \textcite{chenWeakevalStrongEvaluatingEliciting2024} also applied their framework to the \textit{BRAINTEASER} and \textit{RiddleSense} datasets and evaluated open models such as \ac{LLaMA} 3. Their findings demonstrate that incorporating reasoning prompts within their framework improved the performance of various \acp{LLM}. For instance, \ac{LLaMA} 3 (70B) achieved an accuracy of 91.51\% on sentence-based puzzles and 79.54\% on word-based puzzles, illustrating the effectiveness of reasoning-driven approaches in enhancing lateral thinking capabilities~\cite{chenWeakevalStrongEvaluatingEliciting2024}.

\paragraph{Summary.} Collectively, these studies highlight the progression of lateral thinking evaluation. While simpler benchmarks such as \textcite{jiangBRAINTEASERLateralThinking2023} yield higher model performance, more complex frameworks, such as \textcite{huangLatEvalInteractiveLLMs2024} and \textcite{chenWeakevalStrongEvaluatingEliciting2024}, expose the limitations of current models in dynamic interactive reasoning tasks. Although lateral thinking benchmarks have advanced, \acp{LM} continue to perform significantly worse in this domain compared to traditional reasoning tasks.
