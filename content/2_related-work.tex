Lateral thinking evaluation for \acp{LM} has gained significant attention over the past two years, with several key benchmarks contributing to its development. Prior to this surge in interest, much of the focus was on vertical thinking benchmarks and datasets, such as \citewithtitle{biskPIQAReasoningPhysical2019} and \citewithtitle{linRiddleSenseReasoningRiddle2021}, which primarily addressed tasks centered on logical inference and commonsense reasoning. However, lateral thinking, which involves non-linear and creative problem-solving, only began to receive dedicated attention in recent years. One of the earliest efforts in this domain is the \textit{BRAINTEASER} benchmark introduced by \textcite{jiangBRAINTEASERLateralThinking2023}. This benchmark established a structured approach to assess lateral reasoning in \acp{LM}, using a multiple-choice \ac{QA} format to evaluate models on lateral thinking puzzles that differ from traditional logical tasks. The puzzles are categorized into sentence- and word-based types, and evaluation is conducted using both instance- and group-based accuracy metrics, which assess individual puzzle solutions as well as consistency across variations~\cite{jiangBRAINTEASERLateralThinking2023}. Results from this benchmark revealed a performance gap between \acp{LM} and humans, with even the best-performing model, \ac{GPT} 3.5, scoring significantly lower than humans, placing it between human performance and random chance~\cite{jiangBRAINTEASERLateralThinking2023}.

Building on this, \textcite{huangLatEvalInteractiveLLMs2024} developed the \textit{LatEval} framework, which shifts from traditional question-answering tasks to an interactive evaluation system. In this setup, models (acting as "players") ask yes/no questions interactively to extract the necessary information for solving lateral thinking puzzles. Despite the models' ability to ask relevant questions, they struggled to synthesize the gathered information into coherent conclusions, revealing challenges in dynamic reasoning processes~\cite{huangLatEvalInteractiveLLMs2024}.

Similarly, \textcite{chenWeakevalStrongEvaluatingEliciting2024} proposed the \textit{Weak-eval-Strong} framework, utilizing a player-judge evaluation setup with a graded situation puzzles dataset (SPLAT) to assess lateral thinking abilities at varying levels of difficulty. The dataset is divided into three categories: easy, medium, and hard. Models performed well only in the easy category, with \ac{GPT}-4 achieving 70.96\% accuracy~\cite{chenWeakevalStrongEvaluatingEliciting2024}. As the difficulty increased, model performance dropped drastically, with accuracy falling below 40\% for medium difficulty puzzles.

In addition to their own dataset, \textcite{chenWeakevalStrongEvaluatingEliciting2024} evaluated their setup using the \textit{BRAINTEASER} and \textit{RiddleSense} datasets and tested open models like \ac{LLaMA} 3. Their results indicated that reasoning prompts within their framework improved the performance of various \acp{LLM}. For example, \ac{LLaMA} 3 (70B) achieved an overall accuracy of 91.51\% on sentence puzzles and around 79.54\% on word puzzles, demonstrating the effectiveness of their reasoning-driven approach in enhancing lateral thinking capabilities~\cite{chenWeakevalStrongEvaluatingEliciting2024}.

Together, these works highlight the evolution of lateral thinking evaluation. Simpler frameworks, such as \textcite{jiangBRAINTEASERLateralThinking2023}, yield better performance, while more complex systems, such as \textcite{huangLatEvalInteractiveLLMs2024} and \textcite{chenWeakevalStrongEvaluatingEliciting2024}, reveal the limitations of current models in dynamic, creative reasoning tasks. Despite advancements in lateral thinking benchmarks, \acp{LM}'s abilities in this domain remain significantly below their performance in more conventional reasoning tasks.
